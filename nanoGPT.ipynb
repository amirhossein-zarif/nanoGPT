{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9iaN78C19u2",
    "outputId": "71e8e903-7bd2-4268-a3c1-8cfcc4f38bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download one part of dataset from huggingface\n",
    "!wget -O train-00000-of-00016.parquet \"https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/main/data/CC-MAIN-2018-39/train-00000-of-00016.parquet?download=true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset and save the first 100 rows of that to a CSV file\n",
    "import pandas as pd\n",
    "parquet_file_path = 'train-00000-of-00016.parquet'\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "df_first_100 = df.head(100)\n",
    "csv_file_path = 'first_100_rows.csv'\n",
    "df_first_100.to_csv(csv_file_path, index=False)\n",
    "print(f\"First 100 rows have been saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGhxvhho2Oq2",
    "outputId": "5b71c697-919d-4329-c05a-5cd2461cc198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard 0: 100%|██████████| 10000/10000 [00:00<00:00, 2949995.78tokens/s]\n",
      "Shard 1:  92%|█████████▏| 9233/10000 [00:00<00:00, 1806081.93tokens/s]\n",
      "Shard 2:  99%|█████████▉| 9894/10000 [00:00<00:00, 722893.84tokens/s]\n",
      "Shard 3:  86%|████████▋ | 8645/10000 [00:00<00:00, 503922.70tokens/s]\n",
      "Shard 4:  98%|█████████▊| 9840/10000 [00:00<00:00, 2743780.84tokens/s]\n",
      "Shard 5:  93%|█████████▎| 9267/10000 [00:00<00:00, 1011860.96tokens/s]\n",
      "Shard 6:  94%|█████████▍| 9420/10000 [00:00<00:00, 481998.04tokens/s]\n",
      "Shard 7:  98%|█████████▊| 9769/10000 [00:00<00:00, 769511.07tokens/s]\n",
      "Shard 8:  99%|█████████▉| 9899/10000 [00:00<00:00, 929927.77tokens/s]\n",
      "Shard 9:  71%|███████▏  | 7137/10000 [00:00<00:00, 1057914.46tokens/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------\n",
    "local_dir = \"edu_fineweb10B\"\n",
    "shard_size = int(1e4)  # 10000 tokens per shard, total of 10 shards\n",
    "\n",
    "# create the cache the local directory if it doesn't exist yet\n",
    "DATA_CACHE_DIR = os.path.join(os.getcwd(), local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# load the local CSV file\n",
    "csv_file_path = \"first_100_rows.csv\"  # Update this path to your CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "docs = df.to_dict(orient=\"records\")\n",
    "\n",
    "# init the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Define a special end-of-text token\n",
    "eot_token = \"<|endoftext|>\"\n",
    "eot = enc.encode(eot_token, allowed_special={\"<|endoftext|>\"} )[0]\n",
    "\n",
    "def tokenize(doc):\n",
    "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
    "    tokens = [eot]  # the special token delimits all documents\n",
    "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint16\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "\n",
    "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
    "nprocs = max(1, os.cpu_count()//2)\n",
    "with mp.Pool(nprocs) as pool:\n",
    "    shard_index = 0\n",
    "    # preallocate buffer to hold current shard\n",
    "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "    token_count = 0\n",
    "    progress_bar = None\n",
    "    for tokens in pool.imap(tokenize, docs, chunksize=1):\n",
    "        # is there enough space in the current shard for the new tokens?\n",
    "        if token_count + len(tokens) < shard_size:\n",
    "            # simply append tokens to current shard\n",
    "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "            token_count += len(tokens)\n",
    "            # update progress bar\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(len(tokens))\n",
    "        else:\n",
    "            # write the current shard and start a new one\n",
    "            split = \"val\" if shard_index == 0 else \"train\"\n",
    "            filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
    "            remainder = shard_size - token_count\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(remainder)\n",
    "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
    "            write_datafile(filename, all_tokens_np)\n",
    "            shard_index += 1\n",
    "            progress_bar.close()  # Close the progress bar after finishing the shard\n",
    "            progress_bar = None\n",
    "            # populate the next shard with the leftovers of the current doc\n",
    "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
    "            token_count = len(tokens)-remainder\n",
    "\n",
    "    # write any remaining tokens as the last shard\n",
    "    if token_count != 0:\n",
    "        split = \"val\" if shard_index == 0 else \"train\"\n",
    "        filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "        write_datafile(filename, all_tokens_np[:token_count])\n",
    "        if progress_bar is None:\n",
    "            progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "        progress_bar.update(token_count)\n",
    "        progress_bar.close()  # Close the progress bar after finishing the last shard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "G_d_gT1v2A8s",
    "outputId": "64bcbf87-2e29-4865-804a-336e0d2caffc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: colab_kernel_launcher.py [-h] [-m MODEL_TYPE] [-d DEVICE]\n",
      "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-06ab856f-ec44-44a7-a43a-228af623d923.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downloads and evaluates HellaSwag in Python.\n",
    "https://github.com/rowanz/hellaswag\n",
    "\n",
    "Example HellaSwag json item:\n",
    "\n",
    "{\"ind\": 24, \"activity_label\": \"Roof shingle removal\", \"ctx_a\": \"A man is sitting on a roof.\", \"ctx_b\": \"he\", \"ctx\": \"A man is sitting on a roof. he\", \"split\": \"val\", \"split_type\": \"indomain\", \"label\": 3, \"endings\": [\"is using wrap to wrap a pair of skis.\", \"is ripping level tiles off.\", \"is holding a rubik's cube.\", \"starts pulling up roofing on a roof.\"], \"source_id\": \"activitynet~v_-JhWjGDPHMY\"}\n",
    "\n",
    "ind: dataset ID\n",
    "activity_label: The ActivityNet or WikiHow label for this example\n",
    "context: There are two formats. The full context is in ctx. When the context ends in an (incomplete) noun phrase, like for ActivityNet, this incomplete noun phrase is in ctx_b, and the context up until then is in ctx_a. This can be useful for models such as BERT that need the last sentence to be complete. However, it's never required. If ctx_b is nonempty, then ctx is the same thing as ctx_a, followed by a space, then ctx_b.\n",
    "endings: a list of 4 endings. The correct index is given by label (0,1,2, or 3)\n",
    "split: train, val, or test.\n",
    "split_type: indomain if the activity label is seen during training, else zeroshot\n",
    "source_id: Which video or WikiHow article this example came from\n",
    "\n",
    "gpt2 (124M)\n",
    "- eleuther harness reports acc 28.92%, acc_norm 31.14% (multiple choice style)\n",
    "- this script: 10042 acc: 0.2859 acc_norm: 0.2955 (completion style)\n",
    "\n",
    "gpt2-xl (1558M)\n",
    "- eleuther harness reports acc 40.04%, acc_norm 50.89% (multiple choice style)\n",
    "- this script: 10042 acc: 0.3842 acc_norm: 0.4893 (completion style)\n",
    "\n",
    "The validation set of HellaSwag has a total of 10,042 examples.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "DATA_CACHE_DIR = os.path.join(os.getcwd(), \"hellaswag\")\n",
    "\n",
    "def download_file(url: str, fname: str, chunk_size=1024):\n",
    "    \"\"\"Helper function to download a file from a given url\"\"\"\n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get(\"content-length\", 0))\n",
    "    with open(fname, \"wb\") as file, tqdm(\n",
    "        desc=fname,\n",
    "        total=total,\n",
    "        unit=\"iB\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "hellaswags = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
    "    \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
    "}\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def download(split):\n",
    "    \"\"\"Downloads HellaSwag DATA_CACHE_DIR\"\"\"\n",
    "    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "    data_url = hellaswags[split]\n",
    "    data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
    "    if not os.path.exists(data_filename):\n",
    "        print(f\"Downloading {data_url} to {data_filename}...\")\n",
    "        download_file(data_url, data_filename)\n",
    "\n",
    "def render_example(example):\n",
    "    \"\"\"\n",
    "    Given the example as a dictionary, render it as three torch tensors:\n",
    "    - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)\n",
    "    - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)\n",
    "    - label (the index of the correct completion, which we hope has the highest likelihood)\n",
    "    \"\"\"\n",
    "    ctx = example[\"ctx\"]\n",
    "    label = example[\"label\"]\n",
    "    endings = example[\"endings\"]\n",
    "\n",
    "    # data needed to reproduce this eval on the C size\n",
    "    data = {\n",
    "        \"label\": label,\n",
    "        \"ctx_tokens\": None,\n",
    "        \"ending_tokens\": [],\n",
    "    }\n",
    "\n",
    "    # gather up all the tokens\n",
    "    ctx_tokens = enc.encode(ctx)\n",
    "    data[\"ctx_tokens\"] = ctx_tokens\n",
    "    tok_rows = []\n",
    "    mask_rows = []\n",
    "    for end in endings:\n",
    "        end_tokens = enc.encode(\" \" + end) # note: prepending \" \" because GPT-2 tokenizer\n",
    "        tok_rows.append(ctx_tokens + end_tokens)\n",
    "        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
    "        data[\"ending_tokens\"].append(end_tokens)\n",
    "\n",
    "    # have to be careful during the collation because the number of tokens in each row can differ\n",
    "    max_len = max(len(row) for row in tok_rows)\n",
    "    tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    mask = torch.zeros((4, max_len), dtype=torch.long)\n",
    "    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
    "        tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
    "        mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
    "\n",
    "    return data, tokens, mask, label\n",
    "\n",
    "def iterate_examples(split):\n",
    "    # there are 10,042 examples in total in val\n",
    "    download(split)\n",
    "    with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            yield example\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model_type, device):\n",
    "\n",
    "    torch.set_float32_matmul_precision('high') # use tf32\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    model.to(device)\n",
    "    # model = torch.compile(model) # optionally torch compile the model\n",
    "\n",
    "    num_correct_norm = 0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    for example in iterate_examples(\"val\"):\n",
    "        data, tokens, mask, label = render_example(example)\n",
    "        tokens = tokens.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # get the logits\n",
    "        logits = model(tokens).logits\n",
    "        # evaluate the autoregressive loss at all positions\n",
    "        shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "        shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "        flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "        flat_shift_tokens = shift_tokens.view(-1)\n",
    "        shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "        shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "        # now get the average loss just for the completion region (where mask == 1), in each row\n",
    "        shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n",
    "        masked_shift_losses = shift_losses * shift_mask\n",
    "        # sum and divide by the number of 1s in the mask\n",
    "        sum_loss = masked_shift_losses.sum(dim=1)\n",
    "        avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "        # now we have a loss for each of the 4 completions\n",
    "        # the one with the lowest loss should be the most likely\n",
    "        pred = sum_loss.argmin().item()\n",
    "        pred_norm = avg_loss.argmin().item()\n",
    "\n",
    "        # accumulate stats\n",
    "        num_total += 1\n",
    "        num_correct += int(pred == label)\n",
    "        num_correct_norm += int(pred_norm == label)\n",
    "        print(f\"{num_total} acc_norm: {num_correct_norm}/{num_total}={num_correct_norm/num_total:.4f}\")\n",
    "\n",
    "        # debug: pretty print a few examples, and the losses in each case\n",
    "        if num_total < 10:\n",
    "            print(\"---\")\n",
    "            print(f\"Context:\\n {example['ctx']}\")\n",
    "            print(f\"Endings:\")\n",
    "            for i, end in enumerate(example[\"endings\"]):\n",
    "                print(f\"{i} (loss: {avg_loss[i].item():.4f}) {end}\")\n",
    "            print(f\"predicted: {pred_norm}, actual: {label}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-m\", \"--model_type\", type=str, default=\"gpt2\", help=\"the model type to use\")\n",
    "    parser.add_argument(\"-d\", \"--device\", type=str, default=\"cuda\", help=\"the device to use\")\n",
    "    args = parser.parse_args()\n",
    "    evaluate(args.model_type, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGZ1zTZO2Duw",
    "outputId": "ce7d8336-e5da-40bb-deda-acd84935cc9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "total desired batch size: 16384\n",
      "=> calculated gradient accumulation steps: 8\n",
      "found 9 shards for split train\n",
      "found 1 shards for split val\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "validation loss: 10.9298\n",
      "HellaSwag accuracy: 2491/10042=0.2481\n",
      "step     0 | loss: 10.973717 | lr 8.3916e-07 | norm: 15.6005 | dt: 454699.81ms | tok/sec: 36.03\n",
      "step     1 | loss: 10.915951 | lr 1.6783e-06 | norm: 14.5274 | dt: 5194.41ms | tok/sec: 3154.16\n",
      "step     2 | loss: 10.803547 | lr 2.5175e-06 | norm: 14.7551 | dt: 5187.69ms | tok/sec: 3158.25\n",
      "step     3 | loss: 10.686209 | lr 3.3566e-06 | norm: 13.2512 | dt: 5192.75ms | tok/sec: 3155.17\n",
      "step     4 | loss: 10.555614 | lr 4.1958e-06 | norm: 11.4806 | dt: 5185.47ms | tok/sec: 3159.60\n",
      "step     5 | loss: 10.429880 | lr 5.0350e-06 | norm: 8.9251 | dt: 5191.20ms | tok/sec: 3156.11\n",
      "step     6 | loss: 10.255226 | lr 5.8741e-06 | norm: 8.0404 | dt: 5183.33ms | tok/sec: 3160.90\n",
      "step     7 | loss: 10.174383 | lr 6.7133e-06 | norm: 6.9243 | dt: 5182.31ms | tok/sec: 3161.52\n",
      "step     8 | loss: 10.080038 | lr 7.5524e-06 | norm: 6.4724 | dt: 5184.06ms | tok/sec: 3160.46\n",
      "step     9 | loss: 10.040373 | lr 8.3916e-06 | norm: 4.7731 | dt: 5178.69ms | tok/sec: 3163.74\n",
      "step    10 | loss: 9.867546 | lr 9.2308e-06 | norm: 4.5952 | dt: 5178.67ms | tok/sec: 3163.75\n",
      "step    11 | loss: 9.845175 | lr 1.0070e-05 | norm: 3.8640 | dt: 5175.17ms | tok/sec: 3165.88\n",
      "step    12 | loss: 9.759602 | lr 1.0909e-05 | norm: 4.0557 | dt: 5177.61ms | tok/sec: 3164.39\n",
      "step    13 | loss: 9.835035 | lr 1.1748e-05 | norm: 3.0988 | dt: 5163.89ms | tok/sec: 3172.80\n",
      "step    14 | loss: 9.656918 | lr 1.2587e-05 | norm: 3.1333 | dt: 5170.36ms | tok/sec: 3168.83\n",
      "step    15 | loss: 9.693012 | lr 1.3427e-05 | norm: 2.7947 | dt: 5172.48ms | tok/sec: 3167.53\n",
      "step    16 | loss: 9.601231 | lr 1.4266e-05 | norm: 3.1299 | dt: 5172.82ms | tok/sec: 3167.32\n",
      "step    17 | loss: 9.712063 | lr 1.5105e-05 | norm: 2.5875 | dt: 5170.88ms | tok/sec: 3168.51\n",
      "step    18 | loss: 9.568634 | lr 1.5944e-05 | norm: 2.8244 | dt: 5166.35ms | tok/sec: 3171.29\n",
      "step    19 | loss: 9.584898 | lr 1.6783e-05 | norm: 2.6094 | dt: 5172.87ms | tok/sec: 3167.29\n",
      "step    20 | loss: 9.521767 | lr 1.7622e-05 | norm: 2.6274 | dt: 5168.80ms | tok/sec: 3169.79\n",
      "step    21 | loss: 9.570950 | lr 1.8462e-05 | norm: 2.6599 | dt: 5170.94ms | tok/sec: 3168.47\n",
      "step    22 | loss: 9.526268 | lr 1.9301e-05 | norm: 2.6092 | dt: 5164.84ms | tok/sec: 3172.22\n",
      "step    23 | loss: 9.477611 | lr 2.0140e-05 | norm: 2.5682 | dt: 5161.11ms | tok/sec: 3174.51\n",
      "step    24 | loss: 9.405630 | lr 2.0979e-05 | norm: 2.4309 | dt: 5166.91ms | tok/sec: 3170.95\n",
      "step    25 | loss: 9.424118 | lr 2.1818e-05 | norm: 2.9369 | dt: 5164.38ms | tok/sec: 3172.50\n",
      "step    26 | loss: 9.436911 | lr 2.2657e-05 | norm: 2.3794 | dt: 5165.61ms | tok/sec: 3171.75\n",
      "step    27 | loss: 9.345713 | lr 2.3497e-05 | norm: 2.4055 | dt: 5170.55ms | tok/sec: 3168.71\n",
      "step    28 | loss: 9.285503 | lr 2.4336e-05 | norm: 2.4180 | dt: 5165.72ms | tok/sec: 3171.68\n",
      "step    29 | loss: 9.288677 | lr 2.5175e-05 | norm: 3.0080 | dt: 5171.30ms | tok/sec: 3168.25\n",
      "step    30 | loss: 9.295744 | lr 2.6014e-05 | norm: 3.0227 | dt: 5175.92ms | tok/sec: 3165.43\n",
      "step    31 | loss: 9.202943 | lr 2.6853e-05 | norm: 2.4148 | dt: 5180.09ms | tok/sec: 3162.88\n",
      "step    32 | loss: 9.144085 | lr 2.7692e-05 | norm: 2.3133 | dt: 5174.14ms | tok/sec: 3166.52\n",
      "step    33 | loss: 9.131199 | lr 2.8531e-05 | norm: 2.8035 | dt: 5170.48ms | tok/sec: 3168.76\n",
      "step    34 | loss: 9.141361 | lr 2.9371e-05 | norm: 2.5696 | dt: 5176.79ms | tok/sec: 3164.90\n",
      "step    35 | loss: 9.010248 | lr 3.0210e-05 | norm: 2.5280 | dt: 5169.08ms | tok/sec: 3169.61\n",
      "step    36 | loss: 8.979968 | lr 3.1049e-05 | norm: 2.8752 | dt: 5179.12ms | tok/sec: 3163.47\n",
      "step    37 | loss: 8.951876 | lr 3.1888e-05 | norm: 3.3796 | dt: 5176.74ms | tok/sec: 3164.92\n",
      "step    38 | loss: 8.972082 | lr 3.2727e-05 | norm: 3.3089 | dt: 5187.16ms | tok/sec: 3158.57\n",
      "step    39 | loss: 8.798099 | lr 3.3566e-05 | norm: 2.8391 | dt: 5195.81ms | tok/sec: 3153.31\n",
      "step    40 | loss: 8.779306 | lr 3.4406e-05 | norm: 3.0245 | dt: 5190.05ms | tok/sec: 3156.81\n",
      "step    41 | loss: 8.776204 | lr 3.5245e-05 | norm: 3.3779 | dt: 5185.47ms | tok/sec: 3159.60\n",
      "step    42 | loss: 8.851886 | lr 3.6084e-05 | norm: 4.2138 | dt: 5180.41ms | tok/sec: 3162.68\n",
      "step    43 | loss: 8.617948 | lr 3.6923e-05 | norm: 3.4286 | dt: 5182.11ms | tok/sec: 3161.64\n",
      "step    44 | loss: 8.643626 | lr 3.7762e-05 | norm: 12.0519 | dt: 5190.56ms | tok/sec: 3156.50\n",
      "step    45 | loss: 8.600636 | lr 3.8601e-05 | norm: 5.0766 | dt: 5171.09ms | tok/sec: 3168.38\n",
      "step    46 | loss: 8.771090 | lr 3.9441e-05 | norm: 3.2687 | dt: 5175.37ms | tok/sec: 3165.77\n",
      "step    47 | loss: 8.462701 | lr 4.0280e-05 | norm: 3.8269 | dt: 5175.97ms | tok/sec: 3165.40\n",
      "step    48 | loss: 8.499553 | lr 4.1119e-05 | norm: 3.6905 | dt: 5179.02ms | tok/sec: 3163.53\n",
      "step    49 | loss: 8.446528 | lr 4.1958e-05 | norm: 6.9525 | dt: 5179.36ms | tok/sec: 3163.33\n",
      "step    50 | loss: 8.588675 | lr 4.2797e-05 | norm: 4.0167 | dt: 5183.81ms | tok/sec: 3160.61\n",
      "step    51 | loss: 8.319477 | lr 4.3636e-05 | norm: 2.6543 | dt: 5189.25ms | tok/sec: 3157.29\n",
      "step    52 | loss: 8.300806 | lr 4.4476e-05 | norm: 3.6576 | dt: 5179.24ms | tok/sec: 3163.40\n",
      "step    53 | loss: 8.267618 | lr 4.5315e-05 | norm: 3.6983 | dt: 5184.21ms | tok/sec: 3160.36\n",
      "step    54 | loss: 8.346746 | lr 4.6154e-05 | norm: 2.7329 | dt: 5187.47ms | tok/sec: 3158.38\n",
      "step    55 | loss: 8.214643 | lr 4.6993e-05 | norm: 3.9906 | dt: 5183.33ms | tok/sec: 3160.90\n",
      "step    56 | loss: 8.091444 | lr 4.7832e-05 | norm: 3.5496 | dt: 5183.47ms | tok/sec: 3160.82\n",
      "step    57 | loss: 8.110394 | lr 4.8671e-05 | norm: 4.8904 | dt: 5186.11ms | tok/sec: 3159.21\n",
      "step    58 | loss: 8.106165 | lr 4.9510e-05 | norm: 3.8983 | dt: 5192.25ms | tok/sec: 3155.47\n",
      "step    59 | loss: 8.091545 | lr 5.0350e-05 | norm: 7.7853 | dt: 5185.21ms | tok/sec: 3159.76\n",
      "step    60 | loss: 7.928610 | lr 5.1189e-05 | norm: 7.4686 | dt: 5185.93ms | tok/sec: 3159.31\n",
      "step    61 | loss: 7.955477 | lr 5.2028e-05 | norm: 4.5650 | dt: 5194.82ms | tok/sec: 3153.91\n",
      "step    62 | loss: 7.907836 | lr 5.2867e-05 | norm: 4.9223 | dt: 5191.86ms | tok/sec: 3155.71\n",
      "step    63 | loss: 7.904109 | lr 5.3706e-05 | norm: 5.3455 | dt: 5190.58ms | tok/sec: 3156.49\n",
      "step    64 | loss: 7.707304 | lr 5.4545e-05 | norm: 4.4134 | dt: 5191.60ms | tok/sec: 3155.87\n",
      "step    65 | loss: 7.791451 | lr 5.5385e-05 | norm: 5.3484 | dt: 5188.71ms | tok/sec: 3157.62\n",
      "step    66 | loss: 7.697972 | lr 5.6224e-05 | norm: 3.9741 | dt: 5191.47ms | tok/sec: 3155.94\n",
      "step    67 | loss: 7.749622 | lr 5.7063e-05 | norm: 4.9991 | dt: 5190.62ms | tok/sec: 3156.46\n",
      "step    68 | loss: 7.506855 | lr 5.7902e-05 | norm: 6.5342 | dt: 5189.97ms | tok/sec: 3156.86\n",
      "step    69 | loss: 7.621736 | lr 5.8741e-05 | norm: 6.7920 | dt: 5190.00ms | tok/sec: 3156.84\n",
      "step    70 | loss: 7.549242 | lr 5.9580e-05 | norm: 8.4881 | dt: 5192.79ms | tok/sec: 3155.14\n",
      "step    71 | loss: 7.582804 | lr 6.0420e-05 | norm: 4.6305 | dt: 5180.78ms | tok/sec: 3162.46\n",
      "step    72 | loss: 7.367895 | lr 6.1259e-05 | norm: 5.0965 | dt: 5189.96ms | tok/sec: 3156.87\n",
      "step    73 | loss: 7.330477 | lr 6.2098e-05 | norm: 9.3889 | dt: 5183.58ms | tok/sec: 3160.75\n",
      "step    74 | loss: 7.372165 | lr 6.2937e-05 | norm: 5.4728 | dt: 5189.85ms | tok/sec: 3156.93\n",
      "step    75 | loss: 7.393772 | lr 6.3776e-05 | norm: 5.6639 | dt: 5194.22ms | tok/sec: 3154.28\n",
      "step    76 | loss: 7.190411 | lr 6.4615e-05 | norm: 5.3514 | dt: 5185.54ms | tok/sec: 3159.55\n",
      "step    77 | loss: 7.161740 | lr 6.5455e-05 | norm: 8.0132 | dt: 5196.60ms | tok/sec: 3152.83\n",
      "step    78 | loss: 7.159814 | lr 6.6294e-05 | norm: 9.8240 | dt: 5186.29ms | tok/sec: 3159.10\n",
      "step    79 | loss: 7.243372 | lr 6.7133e-05 | norm: 6.8430 | dt: 5191.35ms | tok/sec: 3156.02\n",
      "step    80 | loss: 6.999301 | lr 6.7972e-05 | norm: 9.5938 | dt: 5192.50ms | tok/sec: 3155.32\n",
      "step    81 | loss: 7.053685 | lr 6.8811e-05 | norm: 11.6155 | dt: 5189.88ms | tok/sec: 3156.91\n",
      "step    82 | loss: 6.987301 | lr 6.9650e-05 | norm: 6.0508 | dt: 5192.64ms | tok/sec: 3155.23\n",
      "step    83 | loss: 7.029806 | lr 7.0490e-05 | norm: 6.1639 | dt: 5193.40ms | tok/sec: 3154.77\n",
      "step    84 | loss: 6.876622 | lr 7.1329e-05 | norm: 8.5746 | dt: 5196.59ms | tok/sec: 3152.84\n",
      "step    85 | loss: 6.769978 | lr 7.2168e-05 | norm: 6.7042 | dt: 5194.08ms | tok/sec: 3154.36\n",
      "step    86 | loss: 6.821482 | lr 7.3007e-05 | norm: 9.9086 | dt: 5196.21ms | tok/sec: 3153.06\n",
      "step    87 | loss: 6.780845 | lr 7.3846e-05 | norm: 6.2963 | dt: 5191.61ms | tok/sec: 3155.86\n",
      "step    88 | loss: 6.792492 | lr 7.4685e-05 | norm: 9.4876 | dt: 5191.76ms | tok/sec: 3155.77\n",
      "step    89 | loss: 6.565568 | lr 7.5524e-05 | norm: 8.5455 | dt: 5190.98ms | tok/sec: 3156.24\n",
      "step    90 | loss: 6.720413 | lr 7.6364e-05 | norm: 14.1698 | dt: 5195.44ms | tok/sec: 3153.53\n",
      "step    91 | loss: 6.592326 | lr 7.7203e-05 | norm: 14.1640 | dt: 5191.66ms | tok/sec: 3155.83\n",
      "step    92 | loss: 6.633225 | lr 7.8042e-05 | norm: 5.7776 | dt: 5193.06ms | tok/sec: 3154.98\n",
      "step    93 | loss: 6.377353 | lr 7.8881e-05 | norm: 9.2881 | dt: 5188.40ms | tok/sec: 3157.81\n",
      "step    94 | loss: 6.526117 | lr 7.9720e-05 | norm: 7.5724 | dt: 5188.73ms | tok/sec: 3157.61\n",
      "step    95 | loss: 6.439184 | lr 8.0559e-05 | norm: 9.9161 | dt: 5191.44ms | tok/sec: 3155.96\n",
      "step    96 | loss: 6.565050 | lr 8.1399e-05 | norm: 22.9661 | dt: 5192.46ms | tok/sec: 3155.34\n",
      "step    97 | loss: 6.226468 | lr 8.2238e-05 | norm: 9.3959 | dt: 5193.41ms | tok/sec: 3154.77\n",
      "step    98 | loss: 6.419484 | lr 8.3077e-05 | norm: 8.4869 | dt: 5198.27ms | tok/sec: 3151.82\n",
      "step    99 | loss: 6.342035 | lr 8.3916e-05 | norm: 14.8763 | dt: 5198.10ms | tok/sec: 3151.92\n",
      "step   100 | loss: 6.349258 | lr 8.4755e-05 | norm: 11.2714 | dt: 5193.27ms | tok/sec: 3154.85\n",
      "step   101 | loss: 6.046130 | lr 8.5594e-05 | norm: 12.0723 | dt: 5197.29ms | tok/sec: 3152.41\n",
      "step   102 | loss: 6.229495 | lr 8.6434e-05 | norm: 8.6786 | dt: 5189.87ms | tok/sec: 3156.92\n",
      "step   103 | loss: 6.145240 | lr 8.7273e-05 | norm: 9.1849 | dt: 5189.63ms | tok/sec: 3157.06\n",
      "step   104 | loss: 6.221323 | lr 8.8112e-05 | norm: 10.9109 | dt: 5197.36ms | tok/sec: 3152.37\n",
      "step   105 | loss: 5.953608 | lr 8.8951e-05 | norm: 7.9531 | dt: 5192.34ms | tok/sec: 3155.42\n",
      "step   106 | loss: 5.944477 | lr 8.9790e-05 | norm: 11.5708 | dt: 5188.11ms | tok/sec: 3157.99\n",
      "step   107 | loss: 5.986862 | lr 9.0629e-05 | norm: 8.1576 | dt: 5192.53ms | tok/sec: 3155.30\n",
      "step   108 | loss: 6.067370 | lr 9.1469e-05 | norm: 10.5595 | dt: 5194.35ms | tok/sec: 3154.19\n",
      "step   109 | loss: 5.817960 | lr 9.2308e-05 | norm: 8.2394 | dt: 5194.79ms | tok/sec: 3153.93\n",
      "step   110 | loss: 5.736948 | lr 9.3147e-05 | norm: 7.7652 | dt: 5195.67ms | tok/sec: 3153.39\n",
      "step   111 | loss: 5.858877 | lr 9.3986e-05 | norm: 16.7089 | dt: 5197.49ms | tok/sec: 3152.29\n",
      "step   112 | loss: 5.908926 | lr 9.4825e-05 | norm: 8.0782 | dt: 5193.92ms | tok/sec: 3154.46\n",
      "step   113 | loss: 5.660407 | lr 9.5664e-05 | norm: 11.9114 | dt: 5194.10ms | tok/sec: 3154.35\n",
      "step   114 | loss: 5.606996 | lr 9.6503e-05 | norm: 9.4886 | dt: 5195.25ms | tok/sec: 3153.65\n",
      "step   115 | loss: 5.699835 | lr 9.7343e-05 | norm: 11.4905 | dt: 5192.54ms | tok/sec: 3155.30\n",
      "step   116 | loss: 5.736178 | lr 9.8182e-05 | norm: 10.9981 | dt: 5190.65ms | tok/sec: 3156.45\n",
      "step   117 | loss: 5.567356 | lr 9.9021e-05 | norm: 9.3858 | dt: 5192.40ms | tok/sec: 3155.38\n",
      "step   118 | loss: 5.388231 | lr 9.9860e-05 | norm: 9.0573 | dt: 5193.55ms | tok/sec: 3154.68\n",
      "step   119 | loss: 5.545390 | lr 1.0070e-04 | norm: 11.7392 | dt: 5193.56ms | tok/sec: 3154.68\n",
      "step   120 | loss: 5.543219 | lr 1.0154e-04 | norm: 11.2104 | dt: 5191.72ms | tok/sec: 3155.79\n",
      "step   121 | loss: 5.478827 | lr 1.0238e-04 | norm: 9.5515 | dt: 5202.50ms | tok/sec: 3149.26\n",
      "step   122 | loss: 5.215029 | lr 1.0322e-04 | norm: 11.4355 | dt: 5196.37ms | tok/sec: 3152.97\n",
      "step   123 | loss: 5.403930 | lr 1.0406e-04 | norm: 12.0240 | dt: 5188.93ms | tok/sec: 3157.49\n",
      "step   124 | loss: 5.334060 | lr 1.0490e-04 | norm: 8.7247 | dt: 5199.65ms | tok/sec: 3150.98\n",
      "step   125 | loss: 5.410497 | lr 1.0573e-04 | norm: 14.2756 | dt: 5201.02ms | tok/sec: 3150.15\n",
      "step   126 | loss: 5.125608 | lr 1.0657e-04 | norm: 11.1514 | dt: 5198.77ms | tok/sec: 3151.52\n",
      "step   127 | loss: 5.268514 | lr 1.0741e-04 | norm: 12.7057 | dt: 5192.58ms | tok/sec: 3155.27\n",
      "step   128 | loss: 5.234927 | lr 1.0825e-04 | norm: 11.0614 | dt: 5207.36ms | tok/sec: 3146.31\n",
      "step   129 | loss: 5.215675 | lr 1.0909e-04 | norm: 11.3866 | dt: 5195.28ms | tok/sec: 3153.63\n",
      "step   130 | loss: 4.926226 | lr 1.0993e-04 | norm: 9.5246 | dt: 5207.28ms | tok/sec: 3146.37\n",
      "step   131 | loss: 5.158924 | lr 1.1077e-04 | norm: 13.9907 | dt: 5195.14ms | tok/sec: 3153.71\n",
      "step   132 | loss: 5.131919 | lr 1.1161e-04 | norm: 15.7138 | dt: 5193.58ms | tok/sec: 3154.67\n",
      "step   133 | loss: 5.149649 | lr 1.1245e-04 | norm: 13.2216 | dt: 5196.56ms | tok/sec: 3152.86\n",
      "step   134 | loss: 4.739429 | lr 1.1329e-04 | norm: 9.8764 | dt: 5205.23ms | tok/sec: 3147.61\n",
      "step   135 | loss: 5.038816 | lr 1.1413e-04 | norm: 11.9138 | dt: 5194.63ms | tok/sec: 3154.03\n",
      "step   136 | loss: 4.947365 | lr 1.1497e-04 | norm: 12.5951 | dt: 5209.50ms | tok/sec: 3145.02\n",
      "step   137 | loss: 5.103092 | lr 1.1580e-04 | norm: 13.5025 | dt: 5193.24ms | tok/sec: 3154.87\n",
      "step   138 | loss: 4.768204 | lr 1.1664e-04 | norm: 14.8621 | dt: 5194.40ms | tok/sec: 3154.17\n",
      "step   139 | loss: 4.777992 | lr 1.1748e-04 | norm: 13.1177 | dt: 5195.71ms | tok/sec: 3153.37\n",
      "step   140 | loss: 4.912706 | lr 1.1832e-04 | norm: 16.1645 | dt: 5194.89ms | tok/sec: 3153.87\n",
      "step   141 | loss: 4.975948 | lr 1.1916e-04 | norm: 16.1099 | dt: 5200.54ms | tok/sec: 3150.44\n",
      "step   142 | loss: 4.726700 | lr 1.2000e-04 | norm: 13.8084 | dt: 5194.11ms | tok/sec: 3154.34\n",
      "step   143 | loss: 4.600044 | lr 1.2084e-04 | norm: 10.4120 | dt: 5190.44ms | tok/sec: 3156.57\n",
      "step   144 | loss: 4.773890 | lr 1.2168e-04 | norm: 13.0494 | dt: 5192.09ms | tok/sec: 3155.57\n",
      "step   145 | loss: 4.852247 | lr 1.2252e-04 | norm: 13.9599 | dt: 5191.23ms | tok/sec: 3156.09\n",
      "step   146 | loss: 4.639131 | lr 1.2336e-04 | norm: 16.9259 | dt: 5189.49ms | tok/sec: 3157.15\n",
      "step   147 | loss: 4.515569 | lr 1.2420e-04 | norm: 12.3688 | dt: 5192.10ms | tok/sec: 3155.56\n",
      "step   148 | loss: 4.660511 | lr 1.2503e-04 | norm: 14.1748 | dt: 5193.30ms | tok/sec: 3154.83\n",
      "step   149 | loss: 4.758743 | lr 1.2587e-04 | norm: 15.3026 | dt: 5187.73ms | tok/sec: 3158.22\n",
      "step   150 | loss: 4.575614 | lr 1.2671e-04 | norm: 13.1150 | dt: 5199.87ms | tok/sec: 3150.85\n",
      "step   151 | loss: 4.320282 | lr 1.2755e-04 | norm: 11.9041 | dt: 5193.70ms | tok/sec: 3154.59\n",
      "step   152 | loss: 4.553190 | lr 1.2839e-04 | norm: 13.1565 | dt: 5195.45ms | tok/sec: 3153.53\n",
      "step   153 | loss: 4.554047 | lr 1.2923e-04 | norm: 13.0934 | dt: 5197.16ms | tok/sec: 3152.49\n",
      "step   154 | loss: 4.510932 | lr 1.3007e-04 | norm: 11.6453 | dt: 5188.15ms | tok/sec: 3157.96\n",
      "step   155 | loss: 4.179685 | lr 1.3091e-04 | norm: 12.0382 | dt: 5187.17ms | tok/sec: 3158.56\n",
      "step   156 | loss: 4.411166 | lr 1.3175e-04 | norm: 11.8840 | dt: 5180.29ms | tok/sec: 3162.76\n",
      "step   157 | loss: 4.417068 | lr 1.3259e-04 | norm: 15.5407 | dt: 5187.81ms | tok/sec: 3158.18\n",
      "step   158 | loss: 4.465836 | lr 1.3343e-04 | norm: 14.0441 | dt: 5178.29ms | tok/sec: 3163.98\n",
      "step   159 | loss: 4.147876 | lr 1.3427e-04 | norm: 15.8864 | dt: 5184.55ms | tok/sec: 3160.16\n",
      "step   160 | loss: 4.261696 | lr 1.3510e-04 | norm: 12.3700 | dt: 5186.64ms | tok/sec: 3158.89\n",
      "step   161 | loss: 4.313847 | lr 1.3594e-04 | norm: 13.0874 | dt: 5189.13ms | tok/sec: 3157.37\n",
      "step   162 | loss: 4.288188 | lr 1.3678e-04 | norm: 12.4317 | dt: 5183.65ms | tok/sec: 3160.71\n",
      "step   163 | loss: 3.946356 | lr 1.3762e-04 | norm: 10.6914 | dt: 5192.75ms | tok/sec: 3155.17\n",
      "step   164 | loss: 4.226812 | lr 1.3846e-04 | norm: 16.6014 | dt: 5185.95ms | tok/sec: 3159.31\n",
      "step   165 | loss: 4.185588 | lr 1.3930e-04 | norm: 13.0830 | dt: 5191.11ms | tok/sec: 3156.16\n",
      "step   166 | loss: 4.247001 | lr 1.4014e-04 | norm: 13.7378 | dt: 5192.65ms | tok/sec: 3155.23\n",
      "step   167 | loss: 3.873557 | lr 1.4098e-04 | norm: 15.5348 | dt: 5193.18ms | tok/sec: 3154.91\n",
      "step   168 | loss: 4.079349 | lr 1.4182e-04 | norm: 13.0859 | dt: 5194.85ms | tok/sec: 3153.89\n",
      "step   169 | loss: 4.068737 | lr 1.4266e-04 | norm: 12.9517 | dt: 5203.57ms | tok/sec: 3148.61\n",
      "step   170 | loss: 4.152929 | lr 1.4350e-04 | norm: 13.5345 | dt: 5187.21ms | tok/sec: 3158.54\n",
      "step   171 | loss: 3.933097 | lr 1.4434e-04 | norm: 17.7225 | dt: 5195.93ms | tok/sec: 3153.24\n",
      "step   172 | loss: 3.890543 | lr 1.4517e-04 | norm: 13.9177 | dt: 5194.39ms | tok/sec: 3154.17\n",
      "step   173 | loss: 3.989977 | lr 1.4601e-04 | norm: 14.6501 | dt: 5199.74ms | tok/sec: 3150.92\n",
      "step   174 | loss: 4.086885 | lr 1.4685e-04 | norm: 13.6546 | dt: 5200.02ms | tok/sec: 3150.76\n",
      "step   175 | loss: 3.920211 | lr 1.4769e-04 | norm: 17.1489 | dt: 5191.90ms | tok/sec: 3155.69\n",
      "step   176 | loss: 3.769608 | lr 1.4853e-04 | norm: 14.0485 | dt: 5198.30ms | tok/sec: 3151.80\n",
      "step   177 | loss: 3.901941 | lr 1.4937e-04 | norm: 13.6224 | dt: 5196.56ms | tok/sec: 3152.85\n",
      "step   178 | loss: 3.917171 | lr 1.5021e-04 | norm: 12.4041 | dt: 5195.13ms | tok/sec: 3153.72\n",
      "step   179 | loss: 3.831397 | lr 1.5105e-04 | norm: 16.5809 | dt: 5195.59ms | tok/sec: 3153.44\n",
      "step   180 | loss: 3.646628 | lr 1.5189e-04 | norm: 12.2178 | dt: 5189.40ms | tok/sec: 3157.20\n",
      "step   181 | loss: 3.816980 | lr 1.5273e-04 | norm: 14.1508 | dt: 5195.41ms | tok/sec: 3153.55\n",
      "step   182 | loss: 3.819319 | lr 1.5357e-04 | norm: 14.4072 | dt: 5197.63ms | tok/sec: 3152.20\n",
      "step   183 | loss: 3.713127 | lr 1.5441e-04 | norm: 13.3960 | dt: 5201.67ms | tok/sec: 3149.76\n",
      "step   184 | loss: 3.538269 | lr 1.5524e-04 | norm: 14.6795 | dt: 5187.40ms | tok/sec: 3158.42\n",
      "step   185 | loss: 3.663478 | lr 1.5608e-04 | norm: 13.3336 | dt: 5194.49ms | tok/sec: 3154.11\n",
      "step   186 | loss: 3.657242 | lr 1.5692e-04 | norm: 12.5061 | dt: 5190.51ms | tok/sec: 3156.53\n",
      "step   187 | loss: 3.636827 | lr 1.5776e-04 | norm: 13.2663 | dt: 5185.68ms | tok/sec: 3159.47\n",
      "step   188 | loss: 3.461374 | lr 1.5860e-04 | norm: 14.8749 | dt: 5200.35ms | tok/sec: 3150.56\n",
      "step   189 | loss: 3.533640 | lr 1.5944e-04 | norm: 13.7486 | dt: 5195.96ms | tok/sec: 3153.22\n",
      "step   190 | loss: 3.604294 | lr 1.6028e-04 | norm: 17.5698 | dt: 5199.15ms | tok/sec: 3151.28\n",
      "step   191 | loss: 3.653278 | lr 1.6112e-04 | norm: 15.3320 | dt: 5194.44ms | tok/sec: 3154.14\n",
      "step   192 | loss: 3.330699 | lr 1.6196e-04 | norm: 13.0582 | dt: 5192.46ms | tok/sec: 3155.34\n",
      "step   193 | loss: 3.436889 | lr 1.6280e-04 | norm: 13.1255 | dt: 5190.42ms | tok/sec: 3156.59\n",
      "step   194 | loss: 3.549577 | lr 1.6364e-04 | norm: 18.0624 | dt: 5194.50ms | tok/sec: 3154.10\n",
      "step   195 | loss: 3.519842 | lr 1.6448e-04 | norm: 14.8957 | dt: 5188.75ms | tok/sec: 3157.60\n",
      "step   196 | loss: 3.328461 | lr 1.6531e-04 | norm: 18.4371 | dt: 5187.18ms | tok/sec: 3158.56\n",
      "step   197 | loss: 3.437802 | lr 1.6615e-04 | norm: 14.6911 | dt: 5188.70ms | tok/sec: 3157.63\n",
      "step   198 | loss: 3.369167 | lr 1.6699e-04 | norm: 13.2258 | dt: 5176.67ms | tok/sec: 3164.97\n",
      "step   199 | loss: 3.397049 | lr 1.6783e-04 | norm: 13.6005 | dt: 5185.67ms | tok/sec: 3159.48\n",
      "step   200 | loss: 3.151750 | lr 1.6867e-04 | norm: 14.8834 | dt: 5190.27ms | tok/sec: 3156.67\n",
      "step   201 | loss: 3.324601 | lr 1.6951e-04 | norm: 15.0810 | dt: 5193.50ms | tok/sec: 3154.71\n",
      "step   202 | loss: 3.265463 | lr 1.7035e-04 | norm: 14.8669 | dt: 5188.73ms | tok/sec: 3157.61\n",
      "step   203 | loss: 3.317329 | lr 1.7119e-04 | norm: 13.9740 | dt: 5190.39ms | tok/sec: 3156.60\n",
      "step   204 | loss: 3.188431 | lr 1.7203e-04 | norm: 14.3267 | dt: 5190.70ms | tok/sec: 3156.41\n",
      "step   205 | loss: 3.116513 | lr 1.7287e-04 | norm: 14.9369 | dt: 5185.62ms | tok/sec: 3159.51\n",
      "step   206 | loss: 3.183593 | lr 1.7371e-04 | norm: 14.0323 | dt: 5193.91ms | tok/sec: 3154.46\n",
      "step   207 | loss: 3.278528 | lr 1.7455e-04 | norm: 16.8681 | dt: 5183.90ms | tok/sec: 3160.55\n",
      "step   208 | loss: 3.158847 | lr 1.7538e-04 | norm: 14.0140 | dt: 5192.52ms | tok/sec: 3155.31\n",
      "step   209 | loss: 2.943667 | lr 1.7622e-04 | norm: 11.4646 | dt: 5194.70ms | tok/sec: 3153.98\n",
      "step   210 | loss: 3.118517 | lr 1.7706e-04 | norm: 14.1652 | dt: 5193.21ms | tok/sec: 3154.89\n",
      "step   211 | loss: 3.146334 | lr 1.7790e-04 | norm: 13.3768 | dt: 5192.15ms | tok/sec: 3155.53\n",
      "step   212 | loss: 3.043066 | lr 1.7874e-04 | norm: 14.0157 | dt: 5188.55ms | tok/sec: 3157.72\n",
      "step   213 | loss: 2.935363 | lr 1.7958e-04 | norm: 14.4427 | dt: 5187.95ms | tok/sec: 3158.09\n",
      "step   214 | loss: 2.992635 | lr 1.8042e-04 | norm: 12.0363 | dt: 5196.72ms | tok/sec: 3152.76\n",
      "step   215 | loss: 3.013559 | lr 1.8126e-04 | norm: 14.0381 | dt: 5197.94ms | tok/sec: 3152.02\n",
      "step   216 | loss: 2.977743 | lr 1.8210e-04 | norm: 13.5172 | dt: 5195.26ms | tok/sec: 3153.64\n",
      "step   217 | loss: 2.814701 | lr 1.8294e-04 | norm: 15.5424 | dt: 5195.92ms | tok/sec: 3153.25\n",
      "step   218 | loss: 2.855952 | lr 1.8378e-04 | norm: 12.6228 | dt: 5190.35ms | tok/sec: 3156.63\n",
      "step   219 | loss: 2.885377 | lr 1.8462e-04 | norm: 12.0800 | dt: 5195.83ms | tok/sec: 3153.30\n",
      "step   220 | loss: 2.886898 | lr 1.8545e-04 | norm: 14.9347 | dt: 5192.76ms | tok/sec: 3155.16\n",
      "step   221 | loss: 2.657881 | lr 1.8629e-04 | norm: 12.3675 | dt: 5191.18ms | tok/sec: 3156.12\n",
      "step   222 | loss: 2.757686 | lr 1.8713e-04 | norm: 13.9831 | dt: 5194.10ms | tok/sec: 3154.35\n",
      "step   223 | loss: 2.800508 | lr 1.8797e-04 | norm: 13.1764 | dt: 5198.95ms | tok/sec: 3151.40\n",
      "step   224 | loss: 2.805165 | lr 1.8881e-04 | norm: 14.4297 | dt: 5205.59ms | tok/sec: 3147.39\n",
      "step   225 | loss: 2.650822 | lr 1.8965e-04 | norm: 15.0227 | dt: 5190.72ms | tok/sec: 3156.40\n",
      "step   226 | loss: 2.676268 | lr 1.9049e-04 | norm: 14.7778 | dt: 5206.14ms | tok/sec: 3147.05\n",
      "step   227 | loss: 2.724115 | lr 1.9133e-04 | norm: 13.6042 | dt: 5197.37ms | tok/sec: 3152.37\n",
      "step   228 | loss: 2.684912 | lr 1.9217e-04 | norm: 14.7237 | dt: 5193.47ms | tok/sec: 3154.73\n",
      "step   229 | loss: 2.510703 | lr 1.9301e-04 | norm: 12.3622 | dt: 5200.48ms | tok/sec: 3150.48\n",
      "step   230 | loss: 2.605898 | lr 1.9385e-04 | norm: 14.1778 | dt: 5197.98ms | tok/sec: 3151.99\n",
      "step   231 | loss: 2.610476 | lr 1.9469e-04 | norm: 14.8882 | dt: 5205.62ms | tok/sec: 3147.37\n",
      "step   232 | loss: 2.599424 | lr 1.9552e-04 | norm: 12.1507 | dt: 5200.12ms | tok/sec: 3150.70\n",
      "step   233 | loss: 2.368301 | lr 1.9636e-04 | norm: 12.0048 | dt: 5203.12ms | tok/sec: 3148.88\n",
      "step   234 | loss: 2.459173 | lr 1.9720e-04 | norm: 10.4330 | dt: 5199.36ms | tok/sec: 3151.16\n",
      "step   235 | loss: 2.461209 | lr 1.9804e-04 | norm: 11.5868 | dt: 5190.20ms | tok/sec: 3156.72\n",
      "step   236 | loss: 2.533750 | lr 1.9888e-04 | norm: 12.6338 | dt: 5192.82ms | tok/sec: 3155.13\n",
      "step   237 | loss: 2.356154 | lr 1.9972e-04 | norm: 12.1918 | dt: 5199.88ms | tok/sec: 3150.84\n",
      "step   238 | loss: 2.350052 | lr 2.0056e-04 | norm: 13.4586 | dt: 5208.75ms | tok/sec: 3145.48\n",
      "step   239 | loss: 2.398528 | lr 2.0140e-04 | norm: 11.0011 | dt: 5205.05ms | tok/sec: 3147.71\n",
      "step   240 | loss: 2.418581 | lr 2.0224e-04 | norm: 13.7106 | dt: 5198.54ms | tok/sec: 3151.65\n",
      "step   241 | loss: 2.317557 | lr 2.0308e-04 | norm: 11.3976 | dt: 5198.21ms | tok/sec: 3151.85\n",
      "step   242 | loss: 2.163177 | lr 2.0392e-04 | norm: 10.8663 | dt: 5198.94ms | tok/sec: 3151.41\n",
      "step   243 | loss: 2.302623 | lr 2.0476e-04 | norm: 13.4598 | dt: 5199.88ms | tok/sec: 3150.84\n",
      "step   244 | loss: 2.305911 | lr 2.0559e-04 | norm: 11.9210 | dt: 5201.88ms | tok/sec: 3149.63\n",
      "step   245 | loss: 2.247794 | lr 2.0643e-04 | norm: 13.4988 | dt: 5204.54ms | tok/sec: 3148.02\n",
      "step   246 | loss: 2.071513 | lr 2.0727e-04 | norm: 10.1734 | dt: 5196.59ms | tok/sec: 3152.84\n",
      "step   247 | loss: 2.175285 | lr 2.0811e-04 | norm: 11.0330 | dt: 5196.74ms | tok/sec: 3152.75\n",
      "step   248 | loss: 2.142633 | lr 2.0895e-04 | norm: 10.8049 | dt: 5191.28ms | tok/sec: 3156.06\n",
      "step   249 | loss: 2.142131 | lr 2.0979e-04 | norm: 11.9337 | dt: 5204.52ms | tok/sec: 3148.03\n",
      "validation loss: 8.5071\n",
      "HellaSwag accuracy: 2528/10042=0.2517\n",
      "rank 0 sample 0: Hello, I'm a language model, S has now generally made I his number of the work \"aff are use of the work school this years women— the\n",
      "rank 0 sample 1: Hello, I'm a language model, social work like way by action nowlele I once then you it God and its people up in important areometh\n",
      "rank 0 sample 2: Hello, I'm a language model, a cells cells, much faster than Carand 1 people 1 I would “aanes up in including more more A\n",
      "rank 0 sample 3: Hello, I'm a language model, more 12 spoken languages including--derived into the Dr.\n",
      "- The more must our learning.\n",
      "The they they\n",
      "step   250 | loss: 1.930706 | lr 2.1063e-04 | norm: 10.8689 | dt: 456147.87ms | tok/sec: 35.92\n",
      "step   251 | loss: 2.025597 | lr 2.1147e-04 | norm: 10.7370 | dt: 5192.85ms | tok/sec: 3155.11\n",
      "step   252 | loss: 1.983468 | lr 2.1231e-04 | norm: 8.7099 | dt: 5191.21ms | tok/sec: 3156.11\n",
      "step   253 | loss: 1.997995 | lr 2.1315e-04 | norm: 10.0930 | dt: 5187.81ms | tok/sec: 3158.17\n",
      "step   254 | loss: 1.820513 | lr 2.1399e-04 | norm: 10.5338 | dt: 5192.03ms | tok/sec: 3155.60\n",
      "step   255 | loss: 1.887490 | lr 2.1483e-04 | norm: 10.6442 | dt: 5191.96ms | tok/sec: 3155.65\n",
      "step   256 | loss: 1.915791 | lr 2.1566e-04 | norm: 9.8737 | dt: 5188.93ms | tok/sec: 3157.49\n",
      "step   257 | loss: 1.971881 | lr 2.1650e-04 | norm: 12.3422 | dt: 5193.18ms | tok/sec: 3154.91\n",
      "step   258 | loss: 1.770160 | lr 2.1734e-04 | norm: 10.5156 | dt: 5197.37ms | tok/sec: 3152.36\n",
      "step   259 | loss: 1.780235 | lr 2.1818e-04 | norm: 11.0389 | dt: 5196.92ms | tok/sec: 3152.64\n",
      "step   260 | loss: 1.829198 | lr 2.1902e-04 | norm: 10.5280 | dt: 5196.74ms | tok/sec: 3152.74\n",
      "step   261 | loss: 1.784915 | lr 2.1986e-04 | norm: 11.0754 | dt: 5190.59ms | tok/sec: 3156.48\n",
      "step   262 | loss: 1.640179 | lr 2.2070e-04 | norm: 9.4061 | dt: 5197.73ms | tok/sec: 3152.15\n",
      "step   263 | loss: 1.669236 | lr 2.2154e-04 | norm: 9.6544 | dt: 5199.60ms | tok/sec: 3151.01\n",
      "step   264 | loss: 1.706561 | lr 2.2238e-04 | norm: 9.4298 | dt: 5194.69ms | tok/sec: 3153.99\n",
      "step   265 | loss: 1.755997 | lr 2.2322e-04 | norm: 11.6473 | dt: 5190.82ms | tok/sec: 3156.34\n",
      "step   266 | loss: 1.557217 | lr 2.2406e-04 | norm: 10.2395 | dt: 5192.23ms | tok/sec: 3155.48\n",
      "step   267 | loss: 1.617410 | lr 2.2490e-04 | norm: 10.6480 | dt: 5194.95ms | tok/sec: 3153.83\n",
      "step   268 | loss: 1.601994 | lr 2.2573e-04 | norm: 9.2932 | dt: 5190.41ms | tok/sec: 3156.59\n",
      "step   269 | loss: 1.602789 | lr 2.2657e-04 | norm: 9.1440 | dt: 5188.03ms | tok/sec: 3158.04\n",
      "step   270 | loss: 1.506726 | lr 2.2741e-04 | norm: 9.0465 | dt: 5197.46ms | tok/sec: 3152.31\n",
      "step   271 | loss: 1.389477 | lr 2.2825e-04 | norm: 8.1183 | dt: 5200.04ms | tok/sec: 3150.75\n",
      "step   272 | loss: 1.447469 | lr 2.2909e-04 | norm: 7.8429 | dt: 5202.66ms | tok/sec: 3149.16\n",
      "step   273 | loss: 1.433191 | lr 2.2993e-04 | norm: 8.1294 | dt: 5206.39ms | tok/sec: 3146.90\n",
      "step   274 | loss: 1.399494 | lr 2.3077e-04 | norm: 7.9764 | dt: 5198.51ms | tok/sec: 3151.67\n",
      "step   275 | loss: 1.291140 | lr 2.3161e-04 | norm: 8.2607 | dt: 5195.37ms | tok/sec: 3153.58\n",
      "step   276 | loss: 1.342961 | lr 2.3245e-04 | norm: 7.0072 | dt: 5200.16ms | tok/sec: 3150.67\n",
      "step   277 | loss: 1.291744 | lr 2.3329e-04 | norm: 7.1454 | dt: 5193.52ms | tok/sec: 3154.70\n",
      "step   278 | loss: 1.275475 | lr 2.3413e-04 | norm: 7.0797 | dt: 5205.08ms | tok/sec: 3147.70\n",
      "step   279 | loss: 1.192095 | lr 2.3497e-04 | norm: 7.4681 | dt: 5198.87ms | tok/sec: 3151.46\n",
      "step   280 | loss: 1.241384 | lr 2.3580e-04 | norm: 7.3770 | dt: 5198.36ms | tok/sec: 3151.77\n",
      "step   281 | loss: 1.173188 | lr 2.3664e-04 | norm: 6.7347 | dt: 5205.05ms | tok/sec: 3147.71\n",
      "step   282 | loss: 1.188870 | lr 2.3748e-04 | norm: 7.3971 | dt: 5193.90ms | tok/sec: 3154.47\n",
      "step   283 | loss: 1.087379 | lr 2.3832e-04 | norm: 7.3452 | dt: 5197.74ms | tok/sec: 3152.14\n",
      "step   284 | loss: 1.107486 | lr 2.3916e-04 | norm: 7.1947 | dt: 5196.80ms | tok/sec: 3152.71\n",
      "step   285 | loss: 1.061997 | lr 2.4000e-04 | norm: 6.3661 | dt: 5209.76ms | tok/sec: 3144.87\n",
      "step   286 | loss: 1.099662 | lr 2.4084e-04 | norm: 8.3693 | dt: 5192.52ms | tok/sec: 3155.31\n",
      "step   287 | loss: 0.964623 | lr 2.4168e-04 | norm: 6.0792 | dt: 5193.43ms | tok/sec: 3154.75\n",
      "step   288 | loss: 0.965026 | lr 2.4252e-04 | norm: 6.4287 | dt: 5195.22ms | tok/sec: 3153.67\n",
      "step   289 | loss: 0.954174 | lr 2.4336e-04 | norm: 5.2750 | dt: 5191.08ms | tok/sec: 3156.19\n",
      "step   290 | loss: 0.980200 | lr 2.4420e-04 | norm: 6.4266 | dt: 5202.88ms | tok/sec: 3149.03\n",
      "step   291 | loss: 0.847027 | lr 2.4503e-04 | norm: 5.7738 | dt: 5198.48ms | tok/sec: 3151.69\n",
      "step   292 | loss: 0.842587 | lr 2.4587e-04 | norm: 5.5463 | dt: 5195.34ms | tok/sec: 3153.60\n",
      "step   293 | loss: 0.855638 | lr 2.4671e-04 | norm: 5.9673 | dt: 5202.66ms | tok/sec: 3149.16\n",
      "step   294 | loss: 0.896928 | lr 2.4755e-04 | norm: 6.4769 | dt: 5195.49ms | tok/sec: 3153.50\n",
      "step   295 | loss: 0.760455 | lr 2.4839e-04 | norm: 5.2461 | dt: 5192.54ms | tok/sec: 3155.30\n",
      "step   296 | loss: 0.754994 | lr 2.4923e-04 | norm: 4.9263 | dt: 5190.71ms | tok/sec: 3156.41\n",
      "step   297 | loss: 0.757142 | lr 2.5007e-04 | norm: 5.3014 | dt: 5200.24ms | tok/sec: 3150.62\n",
      "step   298 | loss: 0.772229 | lr 2.5091e-04 | norm: 5.6860 | dt: 5197.00ms | tok/sec: 3152.59\n",
      "step   299 | loss: 0.640403 | lr 2.5175e-04 | norm: 4.6547 | dt: 5204.34ms | tok/sec: 3148.14\n",
      "step   300 | loss: 0.659328 | lr 2.5259e-04 | norm: 4.4683 | dt: 5196.74ms | tok/sec: 3152.75\n",
      "step   301 | loss: 0.649253 | lr 2.5343e-04 | norm: 4.7896 | dt: 5204.28ms | tok/sec: 3148.18\n",
      "step   302 | loss: 0.693046 | lr 2.5427e-04 | norm: 5.3262 | dt: 5202.47ms | tok/sec: 3149.27\n",
      "step   303 | loss: 0.594724 | lr 2.5510e-04 | norm: 4.5200 | dt: 5202.42ms | tok/sec: 3149.30\n",
      "step   304 | loss: 0.526325 | lr 2.5594e-04 | norm: 4.0404 | dt: 5207.73ms | tok/sec: 3146.10\n",
      "step   305 | loss: 0.583001 | lr 2.5678e-04 | norm: 4.4637 | dt: 5196.85ms | tok/sec: 3152.68\n",
      "step   306 | loss: 0.561523 | lr 2.5762e-04 | norm: 4.4257 | dt: 5200.55ms | tok/sec: 3150.44\n",
      "step   307 | loss: 0.525992 | lr 2.5846e-04 | norm: 4.5902 | dt: 5203.99ms | tok/sec: 3148.35\n",
      "step   308 | loss: 0.460971 | lr 2.5930e-04 | norm: 3.8325 | dt: 5195.86ms | tok/sec: 3153.28\n",
      "step   309 | loss: 0.513904 | lr 2.6014e-04 | norm: 3.9498 | dt: 5206.15ms | tok/sec: 3147.05\n",
      "step   310 | loss: 0.459415 | lr 2.6098e-04 | norm: 3.5333 | dt: 5204.09ms | tok/sec: 3148.29\n",
      "step   311 | loss: 0.461830 | lr 2.6182e-04 | norm: 4.0516 | dt: 5205.37ms | tok/sec: 3147.52\n",
      "step   312 | loss: 0.384334 | lr 2.6266e-04 | norm: 3.6219 | dt: 5198.38ms | tok/sec: 3151.75\n",
      "step   313 | loss: 0.434489 | lr 2.6350e-04 | norm: 3.8293 | dt: 5196.57ms | tok/sec: 3152.85\n",
      "step   314 | loss: 0.420291 | lr 2.6434e-04 | norm: 3.8526 | dt: 5194.19ms | tok/sec: 3154.30\n",
      "step   315 | loss: 0.414697 | lr 2.6517e-04 | norm: 3.7664 | dt: 5204.91ms | tok/sec: 3147.80\n",
      "step   316 | loss: 0.369229 | lr 2.6601e-04 | norm: 4.1024 | dt: 5196.42ms | tok/sec: 3152.94\n",
      "step   317 | loss: 0.421616 | lr 2.6685e-04 | norm: 4.1622 | dt: 5208.89ms | tok/sec: 3145.39\n",
      "step   318 | loss: 0.438156 | lr 2.6769e-04 | norm: 3.8217 | dt: 5204.63ms | tok/sec: 3147.97\n",
      "step   319 | loss: 0.384239 | lr 2.6853e-04 | norm: 3.7428 | dt: 5192.06ms | tok/sec: 3155.59\n",
      "step   320 | loss: 0.335294 | lr 2.6937e-04 | norm: 3.3321 | dt: 5206.14ms | tok/sec: 3147.06\n",
      "step   321 | loss: 0.330012 | lr 2.7021e-04 | norm: 3.8667 | dt: 5193.78ms | tok/sec: 3154.54\n",
      "step   322 | loss: 0.349182 | lr 2.7105e-04 | norm: 3.5623 | dt: 5197.52ms | tok/sec: 3152.27\n",
      "step   323 | loss: 0.344030 | lr 2.7189e-04 | norm: 4.0943 | dt: 5203.87ms | tok/sec: 3148.43\n",
      "step   324 | loss: 0.293779 | lr 2.7273e-04 | norm: 3.7159 | dt: 5208.07ms | tok/sec: 3145.89\n",
      "step   325 | loss: 0.317453 | lr 2.7357e-04 | norm: 3.6066 | dt: 5193.36ms | tok/sec: 3154.80\n",
      "step   326 | loss: 0.359165 | lr 2.7441e-04 | norm: 4.3044 | dt: 5196.47ms | tok/sec: 3152.91\n",
      "step   327 | loss: 0.307964 | lr 2.7524e-04 | norm: 4.0044 | dt: 5190.48ms | tok/sec: 3156.55\n",
      "step   328 | loss: 0.265159 | lr 2.7608e-04 | norm: 3.3084 | dt: 5196.69ms | tok/sec: 3152.78\n",
      "step   329 | loss: 0.279763 | lr 2.7692e-04 | norm: 3.7698 | dt: 5200.98ms | tok/sec: 3150.17\n",
      "step   330 | loss: 0.314658 | lr 2.7776e-04 | norm: 4.1280 | dt: 5195.98ms | tok/sec: 3153.21\n",
      "step   331 | loss: 0.320447 | lr 2.7860e-04 | norm: 4.2469 | dt: 5192.84ms | tok/sec: 3155.12\n",
      "step   332 | loss: 0.341350 | lr 2.7944e-04 | norm: 7.0133 | dt: 5190.41ms | tok/sec: 3156.59\n",
      "step   333 | loss: 0.292644 | lr 2.8028e-04 | norm: 3.7704 | dt: 5198.29ms | tok/sec: 3151.81\n",
      "step   334 | loss: 0.292549 | lr 2.8112e-04 | norm: 3.1414 | dt: 5207.46ms | tok/sec: 3146.26\n",
      "step   335 | loss: 0.272140 | lr 2.8196e-04 | norm: 3.6420 | dt: 5199.77ms | tok/sec: 3150.91\n",
      "step   336 | loss: 0.244940 | lr 2.8280e-04 | norm: 3.1440 | dt: 5196.56ms | tok/sec: 3152.86\n",
      "step   337 | loss: 0.229239 | lr 2.8364e-04 | norm: 3.9238 | dt: 5192.63ms | tok/sec: 3155.24\n",
      "step   338 | loss: 0.276758 | lr 2.8448e-04 | norm: 3.6038 | dt: 5195.77ms | tok/sec: 3153.33\n",
      "step   339 | loss: 0.246988 | lr 2.8531e-04 | norm: 3.3947 | dt: 5192.72ms | tok/sec: 3155.19\n",
      "step   340 | loss: 0.253205 | lr 2.8615e-04 | norm: 3.7525 | dt: 5196.77ms | tok/sec: 3152.73\n",
      "step   341 | loss: 0.177007 | lr 2.8699e-04 | norm: 2.2950 | dt: 5204.82ms | tok/sec: 3147.85\n",
      "step   342 | loss: 0.197231 | lr 2.8783e-04 | norm: 2.5951 | dt: 5193.31ms | tok/sec: 3154.83\n",
      "step   343 | loss: 0.173055 | lr 2.8867e-04 | norm: 2.3934 | dt: 5194.83ms | tok/sec: 3153.90\n",
      "step   344 | loss: 0.170819 | lr 2.8951e-04 | norm: 2.3196 | dt: 5203.21ms | tok/sec: 3148.83\n",
      "step   345 | loss: 0.145596 | lr 2.9035e-04 | norm: 1.9801 | dt: 5199.66ms | tok/sec: 3150.97\n",
      "step   346 | loss: 0.171893 | lr 2.9119e-04 | norm: 2.2719 | dt: 5199.02ms | tok/sec: 3151.36\n",
      "step   347 | loss: 0.157154 | lr 2.9203e-04 | norm: 2.3754 | dt: 5204.40ms | tok/sec: 3148.11\n",
      "step   348 | loss: 0.183771 | lr 2.9287e-04 | norm: 2.9101 | dt: 5197.60ms | tok/sec: 3152.22\n",
      "step   349 | loss: 0.147511 | lr 2.9371e-04 | norm: 2.0398 | dt: 5193.94ms | tok/sec: 3154.44\n",
      "step   350 | loss: 0.171043 | lr 2.9455e-04 | norm: 2.7477 | dt: 5198.29ms | tok/sec: 3151.80\n",
      "step   351 | loss: 0.160262 | lr 2.9538e-04 | norm: 2.2195 | dt: 5207.24ms | tok/sec: 3146.39\n",
      "step   352 | loss: 0.144925 | lr 2.9622e-04 | norm: 2.0219 | dt: 5196.03ms | tok/sec: 3153.18\n",
      "step   353 | loss: 0.150684 | lr 2.9706e-04 | norm: 2.8903 | dt: 5204.62ms | tok/sec: 3147.97\n",
      "step   354 | loss: 0.156923 | lr 2.9790e-04 | norm: 2.4111 | dt: 5197.21ms | tok/sec: 3152.46\n",
      "step   355 | loss: 0.175888 | lr 2.9874e-04 | norm: 3.1550 | dt: 5203.57ms | tok/sec: 3148.61\n",
      "step   356 | loss: 0.171674 | lr 2.9958e-04 | norm: 2.5557 | dt: 5194.47ms | tok/sec: 3154.13\n",
      "step   357 | loss: 0.160827 | lr 3.0042e-04 | norm: 2.1993 | dt: 5196.07ms | tok/sec: 3153.15\n",
      "step   358 | loss: 0.144307 | lr 3.0126e-04 | norm: 2.2070 | dt: 5201.95ms | tok/sec: 3149.59\n",
      "step   359 | loss: 0.146234 | lr 3.0210e-04 | norm: 1.9783 | dt: 5206.33ms | tok/sec: 3146.94\n",
      "step   360 | loss: 0.133559 | lr 3.0294e-04 | norm: 1.8640 | dt: 5208.53ms | tok/sec: 3145.61\n",
      "step   361 | loss: 0.124722 | lr 3.0378e-04 | norm: 2.1861 | dt: 5197.52ms | tok/sec: 3152.27\n",
      "step   362 | loss: 0.140094 | lr 3.0462e-04 | norm: 2.2791 | dt: 5199.54ms | tok/sec: 3151.05\n",
      "step   363 | loss: 0.148220 | lr 3.0545e-04 | norm: 1.9097 | dt: 5197.04ms | tok/sec: 3152.56\n",
      "step   364 | loss: 0.119189 | lr 3.0629e-04 | norm: 1.7730 | dt: 5195.87ms | tok/sec: 3153.27\n",
      "step   365 | loss: 0.107500 | lr 3.0713e-04 | norm: 1.6212 | dt: 5203.49ms | tok/sec: 3148.66\n",
      "step   366 | loss: 0.103433 | lr 3.0797e-04 | norm: 1.5438 | dt: 5197.86ms | tok/sec: 3152.07\n",
      "step   367 | loss: 0.106938 | lr 3.0881e-04 | norm: 1.5922 | dt: 5194.40ms | tok/sec: 3154.16\n",
      "step   368 | loss: 0.103819 | lr 3.0965e-04 | norm: 1.4873 | dt: 5204.13ms | tok/sec: 3148.27\n",
      "step   369 | loss: 0.111574 | lr 3.1049e-04 | norm: 1.8840 | dt: 5196.89ms | tok/sec: 3152.65\n",
      "step   370 | loss: 0.110296 | lr 3.1133e-04 | norm: 2.0174 | dt: 5203.10ms | tok/sec: 3148.89\n",
      "step   371 | loss: 0.121481 | lr 3.1217e-04 | norm: 1.7262 | dt: 5197.81ms | tok/sec: 3152.10\n",
      "step   372 | loss: 0.110096 | lr 3.1301e-04 | norm: 1.6008 | dt: 5192.44ms | tok/sec: 3155.36\n",
      "step   373 | loss: 0.102425 | lr 3.1385e-04 | norm: 1.6723 | dt: 5191.84ms | tok/sec: 3155.72\n",
      "step   374 | loss: 0.119132 | lr 3.1469e-04 | norm: 2.0135 | dt: 5196.11ms | tok/sec: 3153.13\n",
      "step   375 | loss: 0.114299 | lr 3.1552e-04 | norm: 1.8020 | dt: 5192.82ms | tok/sec: 3155.13\n",
      "step   376 | loss: 0.097070 | lr 3.1636e-04 | norm: 1.4669 | dt: 5195.76ms | tok/sec: 3153.34\n",
      "step   377 | loss: 0.109518 | lr 3.1720e-04 | norm: 1.7024 | dt: 5197.40ms | tok/sec: 3152.35\n",
      "step   378 | loss: 0.109777 | lr 3.1804e-04 | norm: 1.7058 | dt: 5186.74ms | tok/sec: 3158.83\n",
      "step   379 | loss: 0.105530 | lr 3.1888e-04 | norm: 1.7920 | dt: 5190.45ms | tok/sec: 3156.57\n",
      "step   380 | loss: 0.077280 | lr 3.1972e-04 | norm: 1.3029 | dt: 5189.02ms | tok/sec: 3157.44\n",
      "step   381 | loss: 0.104842 | lr 3.2056e-04 | norm: 1.7499 | dt: 5193.65ms | tok/sec: 3154.62\n",
      "step   382 | loss: 0.128243 | lr 3.2140e-04 | norm: 1.6098 | dt: 5192.59ms | tok/sec: 3155.26\n",
      "step   383 | loss: 0.091969 | lr 3.2224e-04 | norm: 1.5344 | dt: 5187.95ms | tok/sec: 3158.08\n",
      "step   384 | loss: 0.094080 | lr 3.2308e-04 | norm: 1.3597 | dt: 5191.06ms | tok/sec: 3156.19\n",
      "step   385 | loss: 0.088541 | lr 3.2392e-04 | norm: 1.3558 | dt: 5192.32ms | tok/sec: 3155.43\n",
      "step   386 | loss: 0.113466 | lr 3.2476e-04 | norm: 1.3520 | dt: 5189.85ms | tok/sec: 3156.93\n",
      "step   387 | loss: 0.090495 | lr 3.2559e-04 | norm: 1.2827 | dt: 5193.04ms | tok/sec: 3154.99\n",
      "step   388 | loss: 0.084494 | lr 3.2643e-04 | norm: 1.1246 | dt: 5198.75ms | tok/sec: 3151.52\n",
      "step   389 | loss: 0.077980 | lr 3.2727e-04 | norm: 1.3341 | dt: 5190.77ms | tok/sec: 3156.37\n",
      "step   390 | loss: 0.117351 | lr 3.2811e-04 | norm: 1.4539 | dt: 5184.04ms | tok/sec: 3160.47\n",
      "step   391 | loss: 0.076012 | lr 3.2895e-04 | norm: 1.3131 | dt: 5185.25ms | tok/sec: 3159.73\n",
      "step   392 | loss: 0.074854 | lr 3.2979e-04 | norm: 1.0425 | dt: 5190.33ms | tok/sec: 3156.64\n",
      "step   393 | loss: 0.062270 | lr 3.3063e-04 | norm: 0.9843 | dt: 5187.95ms | tok/sec: 3158.09\n",
      "step   394 | loss: 0.075024 | lr 3.3147e-04 | norm: 1.0482 | dt: 5194.99ms | tok/sec: 3153.81\n",
      "step   395 | loss: 0.058520 | lr 3.3231e-04 | norm: 0.9866 | dt: 5184.47ms | tok/sec: 3160.21\n",
      "step   396 | loss: 0.080092 | lr 3.3315e-04 | norm: 1.4668 | dt: 5192.47ms | tok/sec: 3155.34\n",
      "step   397 | loss: 0.070125 | lr 3.3399e-04 | norm: 1.2493 | dt: 5179.42ms | tok/sec: 3163.29\n",
      "step   398 | loss: 0.078976 | lr 3.3483e-04 | norm: 1.2217 | dt: 5190.70ms | tok/sec: 3156.41\n",
      "step   399 | loss: 0.071859 | lr 3.3566e-04 | norm: 1.2711 | dt: 5188.07ms | tok/sec: 3158.01\n",
      "step   400 | loss: 0.077559 | lr 3.3650e-04 | norm: 1.2693 | dt: 5194.07ms | tok/sec: 3154.37\n",
      "step   401 | loss: 0.070553 | lr 3.3734e-04 | norm: 1.1341 | dt: 5196.93ms | tok/sec: 3152.63\n",
      "step   402 | loss: 0.068797 | lr 3.3818e-04 | norm: 1.1813 | dt: 5195.21ms | tok/sec: 3153.67\n",
      "step   403 | loss: 0.079783 | lr 3.3902e-04 | norm: 1.4308 | dt: 5187.52ms | tok/sec: 3158.35\n",
      "step   404 | loss: 0.105638 | lr 3.3986e-04 | norm: 1.9541 | dt: 5187.31ms | tok/sec: 3158.48\n",
      "step   405 | loss: 0.115257 | lr 3.4070e-04 | norm: 2.4769 | dt: 5195.68ms | tok/sec: 3153.39\n",
      "step   406 | loss: 0.107787 | lr 3.4154e-04 | norm: 1.6913 | dt: 5191.92ms | tok/sec: 3155.67\n",
      "step   407 | loss: 0.102555 | lr 3.4238e-04 | norm: 1.2870 | dt: 5187.53ms | tok/sec: 3158.34\n",
      "step   408 | loss: 0.084995 | lr 3.4322e-04 | norm: 1.2099 | dt: 5190.80ms | tok/sec: 3156.35\n",
      "step   409 | loss: 0.084888 | lr 3.4406e-04 | norm: 1.3939 | dt: 5193.86ms | tok/sec: 3154.50\n",
      "step   410 | loss: 0.075305 | lr 3.4490e-04 | norm: 1.4334 | dt: 5196.23ms | tok/sec: 3153.06\n",
      "step   411 | loss: 0.068048 | lr 3.4573e-04 | norm: 1.0056 | dt: 5190.88ms | tok/sec: 3156.30\n",
      "step   412 | loss: 0.063461 | lr 3.4657e-04 | norm: 1.0774 | dt: 5195.30ms | tok/sec: 3153.62\n",
      "step   413 | loss: 0.075720 | lr 3.4741e-04 | norm: 0.9953 | dt: 5193.75ms | tok/sec: 3154.56\n",
      "step   414 | loss: 0.065693 | lr 3.4825e-04 | norm: 1.4002 | dt: 5197.36ms | tok/sec: 3152.37\n",
      "step   415 | loss: 0.069626 | lr 3.4909e-04 | norm: 1.1741 | dt: 5190.56ms | tok/sec: 3156.50\n",
      "step   416 | loss: 0.065962 | lr 3.4993e-04 | norm: 1.2136 | dt: 5192.22ms | tok/sec: 3155.49\n",
      "step   417 | loss: 0.066899 | lr 3.5077e-04 | norm: 1.1194 | dt: 5191.14ms | tok/sec: 3156.15\n",
      "step   418 | loss: 0.074431 | lr 3.5161e-04 | norm: 1.2752 | dt: 5193.51ms | tok/sec: 3154.70\n",
      "step   419 | loss: 0.065889 | lr 3.5245e-04 | norm: 1.1423 | dt: 5193.50ms | tok/sec: 3154.71\n",
      "step   420 | loss: 0.056470 | lr 3.5329e-04 | norm: 1.0085 | dt: 5205.05ms | tok/sec: 3147.71\n",
      "step   421 | loss: 0.070801 | lr 3.5413e-04 | norm: 1.0637 | dt: 5198.93ms | tok/sec: 3151.42\n",
      "step   422 | loss: 0.066629 | lr 3.5497e-04 | norm: 1.0168 | dt: 5196.28ms | tok/sec: 3153.03\n",
      "step   423 | loss: 0.066747 | lr 3.5580e-04 | norm: 0.9092 | dt: 5205.59ms | tok/sec: 3147.38\n",
      "step   424 | loss: 0.043233 | lr 3.5664e-04 | norm: 0.7838 | dt: 5206.32ms | tok/sec: 3146.94\n",
      "step   425 | loss: 0.054813 | lr 3.5748e-04 | norm: 0.8813 | dt: 5198.82ms | tok/sec: 3151.48\n",
      "step   426 | loss: 0.071709 | lr 3.5832e-04 | norm: 1.2118 | dt: 5198.22ms | tok/sec: 3151.85\n",
      "step   427 | loss: 0.065823 | lr 3.5916e-04 | norm: 1.0262 | dt: 5196.30ms | tok/sec: 3153.01\n",
      "step   428 | loss: 0.047880 | lr 3.6000e-04 | norm: 0.8979 | dt: 5194.74ms | tok/sec: 3153.96\n",
      "step   429 | loss: 0.071771 | lr 3.6084e-04 | norm: 1.2149 | dt: 5197.36ms | tok/sec: 3152.37\n",
      "step   430 | loss: 0.117480 | lr 3.6168e-04 | norm: 1.4715 | dt: 5204.01ms | tok/sec: 3148.34\n",
      "step   431 | loss: 0.066667 | lr 3.6252e-04 | norm: 1.0498 | dt: 5209.86ms | tok/sec: 3144.80\n",
      "step   432 | loss: 0.053537 | lr 3.6336e-04 | norm: 0.9340 | dt: 5191.75ms | tok/sec: 3155.78\n",
      "step   433 | loss: 0.052826 | lr 3.6420e-04 | norm: 0.9112 | dt: 5198.08ms | tok/sec: 3151.94\n",
      "step   434 | loss: 0.073160 | lr 3.6503e-04 | norm: 1.0270 | dt: 5199.52ms | tok/sec: 3151.06\n",
      "step   435 | loss: 0.055810 | lr 3.6587e-04 | norm: 1.0073 | dt: 5201.41ms | tok/sec: 3149.91\n",
      "step   436 | loss: 0.060020 | lr 3.6671e-04 | norm: 0.9700 | dt: 5201.27ms | tok/sec: 3150.00\n",
      "step   437 | loss: 0.084416 | lr 3.6755e-04 | norm: 1.4379 | dt: 5197.75ms | tok/sec: 3152.13\n",
      "step   438 | loss: 0.067373 | lr 3.6839e-04 | norm: 1.1470 | dt: 5197.48ms | tok/sec: 3152.30\n",
      "step   439 | loss: 0.059096 | lr 3.6923e-04 | norm: 1.0594 | dt: 5194.29ms | tok/sec: 3154.23\n",
      "step   440 | loss: 0.073778 | lr 3.7007e-04 | norm: 1.8033 | dt: 5194.14ms | tok/sec: 3154.33\n",
      "step   441 | loss: 0.086351 | lr 3.7091e-04 | norm: 1.5006 | dt: 5196.65ms | tok/sec: 3152.80\n",
      "step   442 | loss: 0.077118 | lr 3.7175e-04 | norm: 0.9184 | dt: 5190.04ms | tok/sec: 3156.82\n",
      "step   443 | loss: 0.047806 | lr 3.7259e-04 | norm: 0.8391 | dt: 5195.66ms | tok/sec: 3153.40\n",
      "step   444 | loss: 0.059731 | lr 3.7343e-04 | norm: 1.0530 | dt: 5188.85ms | tok/sec: 3157.54\n",
      "step   445 | loss: 0.076223 | lr 3.7427e-04 | norm: 1.0561 | dt: 5191.66ms | tok/sec: 3155.83\n",
      "step   446 | loss: 0.041417 | lr 3.7510e-04 | norm: 0.6445 | dt: 5188.48ms | tok/sec: 3157.77\n",
      "step   447 | loss: 0.043126 | lr 3.7594e-04 | norm: 0.7807 | dt: 5196.68ms | tok/sec: 3152.78\n",
      "step   448 | loss: 0.040622 | lr 3.7678e-04 | norm: 0.7668 | dt: 5204.83ms | tok/sec: 3147.84\n",
      "step   449 | loss: 0.037865 | lr 3.7762e-04 | norm: 0.6810 | dt: 5189.01ms | tok/sec: 3157.44\n",
      "step   450 | loss: 0.040880 | lr 3.7846e-04 | norm: 0.6461 | dt: 5191.90ms | tok/sec: 3155.69\n",
      "step   451 | loss: 0.030358 | lr 3.7930e-04 | norm: 0.6319 | dt: 5198.19ms | tok/sec: 3151.87\n",
      "step   452 | loss: 0.036508 | lr 3.8014e-04 | norm: 0.7122 | dt: 5196.18ms | tok/sec: 3153.09\n",
      "step   453 | loss: 0.036663 | lr 3.8098e-04 | norm: 0.7737 | dt: 5199.47ms | tok/sec: 3151.09\n",
      "step   454 | loss: 0.034468 | lr 3.8182e-04 | norm: 0.6154 | dt: 5194.32ms | tok/sec: 3154.21\n",
      "step   455 | loss: 0.032552 | lr 3.8266e-04 | norm: 0.7127 | dt: 5200.30ms | tok/sec: 3150.58\n",
      "step   456 | loss: 0.030428 | lr 3.8350e-04 | norm: 0.6778 | dt: 5201.76ms | tok/sec: 3149.70\n",
      "step   457 | loss: 0.038366 | lr 3.8434e-04 | norm: 0.7800 | dt: 5204.03ms | tok/sec: 3148.33\n",
      "step   458 | loss: 0.032282 | lr 3.8517e-04 | norm: 0.6063 | dt: 5194.74ms | tok/sec: 3153.96\n",
      "step   459 | loss: 0.035600 | lr 3.8601e-04 | norm: 0.7755 | dt: 5206.48ms | tok/sec: 3146.85\n",
      "step   460 | loss: 0.029721 | lr 3.8685e-04 | norm: 0.7375 | dt: 5197.73ms | tok/sec: 3152.14\n",
      "step   461 | loss: 0.036596 | lr 3.8769e-04 | norm: 0.7817 | dt: 5200.26ms | tok/sec: 3150.61\n",
      "step   462 | loss: 0.045837 | lr 3.8853e-04 | norm: 1.1610 | dt: 5204.14ms | tok/sec: 3148.26\n",
      "step   463 | loss: 0.055011 | lr 3.8937e-04 | norm: 1.0618 | dt: 5205.74ms | tok/sec: 3147.30\n",
      "step   464 | loss: 0.041244 | lr 3.9021e-04 | norm: 0.8419 | dt: 5200.49ms | tok/sec: 3150.47\n",
      "step   465 | loss: 0.040757 | lr 3.9105e-04 | norm: 0.8779 | dt: 5203.93ms | tok/sec: 3148.39\n",
      "step   466 | loss: 0.038595 | lr 3.9189e-04 | norm: 0.7314 | dt: 5200.50ms | tok/sec: 3150.47\n",
      "step   467 | loss: 0.053828 | lr 3.9273e-04 | norm: 1.2350 | dt: 5196.56ms | tok/sec: 3152.85\n",
      "step   468 | loss: 0.086825 | lr 3.9357e-04 | norm: 1.8401 | dt: 5193.56ms | tok/sec: 3154.68\n",
      "step   469 | loss: 0.060498 | lr 3.9441e-04 | norm: 1.0988 | dt: 5195.40ms | tok/sec: 3153.56\n",
      "step   470 | loss: 0.046968 | lr 3.9524e-04 | norm: 0.8260 | dt: 5206.25ms | tok/sec: 3146.99\n",
      "step   471 | loss: 0.057295 | lr 3.9608e-04 | norm: 0.8933 | dt: 5199.21ms | tok/sec: 3151.25\n",
      "step   472 | loss: 0.052880 | lr 3.9692e-04 | norm: 0.9101 | dt: 5210.10ms | tok/sec: 3144.66\n",
      "step   473 | loss: 0.044273 | lr 3.9776e-04 | norm: 0.7978 | dt: 5190.61ms | tok/sec: 3156.47\n",
      "step   474 | loss: 0.045014 | lr 3.9860e-04 | norm: 0.7397 | dt: 5190.11ms | tok/sec: 3156.77\n",
      "step   475 | loss: 0.044278 | lr 3.9944e-04 | norm: 0.7599 | dt: 5200.72ms | tok/sec: 3150.33\n",
      "step   476 | loss: 0.043725 | lr 4.0028e-04 | norm: 0.7598 | dt: 5196.47ms | tok/sec: 3152.91\n",
      "step   477 | loss: 0.039342 | lr 4.0112e-04 | norm: 0.7138 | dt: 5194.56ms | tok/sec: 3154.07\n",
      "step   478 | loss: 0.034461 | lr 4.0196e-04 | norm: 0.6799 | dt: 5196.14ms | tok/sec: 3153.11\n",
      "step   479 | loss: 0.040943 | lr 4.0280e-04 | norm: 0.7409 | dt: 5196.20ms | tok/sec: 3153.08\n",
      "step   480 | loss: 0.044690 | lr 4.0364e-04 | norm: 0.7869 | dt: 5195.10ms | tok/sec: 3153.74\n",
      "step   481 | loss: 0.038481 | lr 4.0448e-04 | norm: 0.7416 | dt: 5193.99ms | tok/sec: 3154.41\n",
      "step   482 | loss: 0.046073 | lr 4.0531e-04 | norm: 0.7934 | dt: 5191.23ms | tok/sec: 3156.09\n",
      "step   483 | loss: 0.041501 | lr 4.0615e-04 | norm: 0.8625 | dt: 5189.77ms | tok/sec: 3156.98\n",
      "step   484 | loss: 0.038675 | lr 4.0699e-04 | norm: 0.7038 | dt: 5193.87ms | tok/sec: 3154.49\n",
      "step   485 | loss: 0.045806 | lr 4.0783e-04 | norm: 0.8014 | dt: 5190.77ms | tok/sec: 3156.37\n",
      "step   486 | loss: 0.041314 | lr 4.0867e-04 | norm: 0.7099 | dt: 5190.51ms | tok/sec: 3156.53\n",
      "step   487 | loss: 0.038327 | lr 4.0951e-04 | norm: 0.6864 | dt: 5189.18ms | tok/sec: 3157.34\n",
      "step   488 | loss: 0.040811 | lr 4.1035e-04 | norm: 0.7613 | dt: 5190.79ms | tok/sec: 3156.36\n",
      "step   489 | loss: 0.038110 | lr 4.1119e-04 | norm: 0.7829 | dt: 5193.74ms | tok/sec: 3154.57\n",
      "step   490 | loss: 0.038586 | lr 4.1203e-04 | norm: 0.7257 | dt: 5191.79ms | tok/sec: 3155.75\n",
      "step   491 | loss: 0.030198 | lr 4.1287e-04 | norm: 0.6145 | dt: 5197.13ms | tok/sec: 3152.51\n",
      "step   492 | loss: 0.035051 | lr 4.1371e-04 | norm: 0.6775 | dt: 5191.98ms | tok/sec: 3155.64\n",
      "step   493 | loss: 0.038617 | lr 4.1455e-04 | norm: 0.7635 | dt: 5195.18ms | tok/sec: 3153.69\n",
      "step   494 | loss: 0.036642 | lr 4.1538e-04 | norm: 0.6660 | dt: 5191.81ms | tok/sec: 3155.74\n",
      "step   495 | loss: 0.036718 | lr 4.1622e-04 | norm: 0.7052 | dt: 5191.95ms | tok/sec: 3155.65\n",
      "step   496 | loss: 0.041576 | lr 4.1706e-04 | norm: 0.7290 | dt: 5194.13ms | tok/sec: 3154.33\n",
      "step   497 | loss: 0.040211 | lr 4.1790e-04 | norm: 0.7388 | dt: 5186.19ms | tok/sec: 3159.16\n",
      "step   498 | loss: 0.033349 | lr 4.1874e-04 | norm: 0.6676 | dt: 5193.25ms | tok/sec: 3154.86\n",
      "step   499 | loss: 0.039478 | lr 4.1958e-04 | norm: 0.6815 | dt: 5186.22ms | tok/sec: 3159.14\n",
      "validation loss: 9.4161\n",
      "HellaSwag accuracy: 2514/10042=0.2503\n",
      "rank 0 sample 0: Hello, I'm a language model, occur they. A key species is drugs onlyü species that doesn housing financial primary source?I doesn’s key\n",
      "rank 0 sample 1: Hello, I'm a language model,'s length of as and typically new rub even style students died called. wetlands and species may be kept by his five species\n",
      "rank 0 sample 2: Hello, I'm a language model,words of freshwaters and because of Massacre’s were r our and 6, which hard were de dend attitude\n",
      "rank 0 sample 3: Hello, I'm a language model,;ated SEE created after species species skill'tisOnMy flees of exact success were rescued part of as modes four\n",
      "step   500 | loss: 0.061213 | lr 4.2042e-04 | norm: 1.1491 | dt: 455817.73ms | tok/sec: 35.94\n",
      "step   501 | loss: 0.039287 | lr 4.2126e-04 | norm: 0.6478 | dt: 5186.77ms | tok/sec: 3158.81\n",
      "step   502 | loss: 0.037226 | lr 4.2210e-04 | norm: 0.6165 | dt: 5190.40ms | tok/sec: 3156.60\n",
      "step   503 | loss: 0.052902 | lr 4.2294e-04 | norm: 0.6440 | dt: 5193.22ms | tok/sec: 3154.88\n",
      "step   504 | loss: 0.035138 | lr 4.2378e-04 | norm: 0.5737 | dt: 5196.60ms | tok/sec: 3152.83\n",
      "step   505 | loss: 0.029689 | lr 4.2462e-04 | norm: 0.5242 | dt: 5187.66ms | tok/sec: 3158.26\n",
      "step   506 | loss: 0.035110 | lr 4.2545e-04 | norm: 0.6398 | dt: 5188.07ms | tok/sec: 3158.01\n",
      "step   507 | loss: 0.031467 | lr 4.2629e-04 | norm: 0.7106 | dt: 5193.04ms | tok/sec: 3154.99\n",
      "step   508 | loss: 0.032985 | lr 4.2713e-04 | norm: 0.6367 | dt: 5184.29ms | tok/sec: 3160.32\n",
      "step   509 | loss: 0.024708 | lr 4.2797e-04 | norm: 0.4907 | dt: 5192.58ms | tok/sec: 3155.27\n",
      "step   510 | loss: 0.029608 | lr 4.2881e-04 | norm: 0.6661 | dt: 5189.45ms | tok/sec: 3157.17\n",
      "step   511 | loss: 0.093931 | lr 4.2965e-04 | norm: 1.2444 | dt: 5196.54ms | tok/sec: 3152.87\n",
      "step   512 | loss: 0.076127 | lr 4.3049e-04 | norm: 1.5662 | dt: 5194.97ms | tok/sec: 3153.82\n",
      "step   513 | loss: 0.043435 | lr 4.3133e-04 | norm: 0.7629 | dt: 5190.62ms | tok/sec: 3156.46\n",
      "step   514 | loss: 0.032900 | lr 4.3217e-04 | norm: 0.5310 | dt: 5196.88ms | tok/sec: 3152.66\n",
      "step   515 | loss: 0.083203 | lr 4.3301e-04 | norm: 1.8798 | dt: 5197.63ms | tok/sec: 3152.21\n",
      "step   516 | loss: 0.124326 | lr 4.3385e-04 | norm: 1.3743 | dt: 5202.38ms | tok/sec: 3149.33\n",
      "step   517 | loss: 0.057060 | lr 4.3469e-04 | norm: 0.9187 | dt: 5190.34ms | tok/sec: 3156.64\n",
      "step   518 | loss: 0.062814 | lr 4.3552e-04 | norm: 1.2821 | dt: 5189.82ms | tok/sec: 3156.95\n",
      "step   519 | loss: 0.067895 | lr 4.3636e-04 | norm: 0.9755 | dt: 5192.39ms | tok/sec: 3155.39\n",
      "step   520 | loss: 0.049274 | lr 4.3720e-04 | norm: 0.6329 | dt: 5198.24ms | tok/sec: 3151.84\n",
      "step   521 | loss: 0.033968 | lr 4.3804e-04 | norm: 0.5381 | dt: 5194.99ms | tok/sec: 3153.81\n",
      "step   522 | loss: 0.032283 | lr 4.3888e-04 | norm: 0.6070 | dt: 5194.63ms | tok/sec: 3154.02\n",
      "step   523 | loss: 0.043058 | lr 4.3972e-04 | norm: 0.7703 | dt: 5200.42ms | tok/sec: 3150.52\n",
      "step   524 | loss: 0.032487 | lr 4.4056e-04 | norm: 0.5325 | dt: 5194.12ms | tok/sec: 3154.33\n",
      "step   525 | loss: 0.026395 | lr 4.4140e-04 | norm: 0.4901 | dt: 5194.54ms | tok/sec: 3154.08\n",
      "step   526 | loss: 0.029275 | lr 4.4224e-04 | norm: 0.5139 | dt: 5199.35ms | tok/sec: 3151.16\n",
      "step   527 | loss: 0.036746 | lr 4.4308e-04 | norm: 0.5052 | dt: 5199.30ms | tok/sec: 3151.19\n",
      "step   528 | loss: 0.026951 | lr 4.4392e-04 | norm: 0.5156 | dt: 5196.72ms | tok/sec: 3152.76\n",
      "step   529 | loss: 0.020882 | lr 4.4476e-04 | norm: 0.3877 | dt: 5200.33ms | tok/sec: 3150.57\n",
      "step   530 | loss: 0.026389 | lr 4.4559e-04 | norm: 0.6111 | dt: 5199.51ms | tok/sec: 3151.07\n",
      "step   531 | loss: 0.026985 | lr 4.4643e-04 | norm: 0.6144 | dt: 5201.17ms | tok/sec: 3150.06\n",
      "step   532 | loss: 0.027047 | lr 4.4727e-04 | norm: 0.4960 | dt: 5197.22ms | tok/sec: 3152.45\n",
      "step   533 | loss: 0.031759 | lr 4.4811e-04 | norm: 0.6200 | dt: 5201.53ms | tok/sec: 3149.84\n",
      "step   534 | loss: 0.028701 | lr 4.4895e-04 | norm: 0.5023 | dt: 5201.06ms | tok/sec: 3150.12\n",
      "step   535 | loss: 0.030869 | lr 4.4979e-04 | norm: 0.5830 | dt: 5207.38ms | tok/sec: 3146.30\n",
      "step   536 | loss: 0.032143 | lr 4.5063e-04 | norm: 0.9621 | dt: 5195.19ms | tok/sec: 3153.68\n",
      "step   537 | loss: 0.033138 | lr 4.5147e-04 | norm: 0.6066 | dt: 5192.50ms | tok/sec: 3155.32\n",
      "step   538 | loss: 0.035223 | lr 4.5231e-04 | norm: 0.5688 | dt: 5186.44ms | tok/sec: 3159.00\n",
      "step   539 | loss: 0.037698 | lr 4.5315e-04 | norm: 0.6565 | dt: 5188.50ms | tok/sec: 3157.75\n",
      "step   540 | loss: 0.036915 | lr 4.5399e-04 | norm: 0.6552 | dt: 5195.11ms | tok/sec: 3153.74\n",
      "step   541 | loss: 0.026446 | lr 4.5483e-04 | norm: 0.5166 | dt: 5184.96ms | tok/sec: 3159.91\n",
      "step   542 | loss: 0.027495 | lr 4.5566e-04 | norm: 0.5578 | dt: 5198.24ms | tok/sec: 3151.84\n",
      "step   543 | loss: 0.033125 | lr 4.5650e-04 | norm: 0.6161 | dt: 5193.12ms | tok/sec: 3154.95\n",
      "step   544 | loss: 0.027138 | lr 4.5734e-04 | norm: 0.5069 | dt: 5188.25ms | tok/sec: 3157.90\n",
      "step   545 | loss: 0.030466 | lr 4.5818e-04 | norm: 0.5637 | dt: 5190.88ms | tok/sec: 3156.30\n",
      "step   546 | loss: 0.024310 | lr 4.5902e-04 | norm: 0.4378 | dt: 5193.73ms | tok/sec: 3154.57\n",
      "step   547 | loss: 0.047392 | lr 4.5986e-04 | norm: 0.7141 | dt: 5193.58ms | tok/sec: 3154.66\n",
      "step   548 | loss: 0.035701 | lr 4.6070e-04 | norm: 0.7061 | dt: 5201.39ms | tok/sec: 3149.93\n",
      "step   549 | loss: 0.029429 | lr 4.6154e-04 | norm: 0.4764 | dt: 5193.75ms | tok/sec: 3154.56\n",
      "step   550 | loss: 0.033301 | lr 4.6238e-04 | norm: 0.7002 | dt: 5190.20ms | tok/sec: 3156.72\n",
      "step   551 | loss: 0.039845 | lr 4.6322e-04 | norm: 0.6366 | dt: 5189.61ms | tok/sec: 3157.08\n",
      "step   552 | loss: 0.029248 | lr 4.6406e-04 | norm: 0.5718 | dt: 5189.44ms | tok/sec: 3157.18\n",
      "step   553 | loss: 0.026044 | lr 4.6490e-04 | norm: 0.4834 | dt: 5188.01ms | tok/sec: 3158.05\n",
      "step   554 | loss: 0.035842 | lr 4.6573e-04 | norm: 0.6862 | dt: 5191.94ms | tok/sec: 3155.66\n",
      "step   555 | loss: 0.077606 | lr 4.6657e-04 | norm: 1.5599 | dt: 5187.78ms | tok/sec: 3158.19\n",
      "step   556 | loss: 0.048866 | lr 4.6741e-04 | norm: 0.9561 | dt: 5190.89ms | tok/sec: 3156.30\n",
      "step   557 | loss: 0.025793 | lr 4.6825e-04 | norm: 0.4349 | dt: 5194.62ms | tok/sec: 3154.03\n",
      "step   558 | loss: 0.042063 | lr 4.6909e-04 | norm: 0.7461 | dt: 5192.15ms | tok/sec: 3155.53\n",
      "step   559 | loss: 0.038325 | lr 4.6993e-04 | norm: 0.6755 | dt: 5192.00ms | tok/sec: 3155.62\n",
      "step   560 | loss: 0.072562 | lr 4.7077e-04 | norm: 1.7865 | dt: 5186.05ms | tok/sec: 3159.25\n",
      "step   561 | loss: 0.088923 | lr 4.7161e-04 | norm: 1.6328 | dt: 5186.08ms | tok/sec: 3159.23\n",
      "step   562 | loss: 0.074398 | lr 4.7245e-04 | norm: 0.9758 | dt: 5195.09ms | tok/sec: 3153.75\n",
      "step   563 | loss: 0.065496 | lr 4.7329e-04 | norm: 1.1262 | dt: 5196.00ms | tok/sec: 3153.19\n",
      "step   564 | loss: 0.056632 | lr 4.7413e-04 | norm: 0.9075 | dt: 5187.74ms | tok/sec: 3158.21\n",
      "step   565 | loss: 0.040460 | lr 4.7497e-04 | norm: 0.7139 | dt: 5196.74ms | tok/sec: 3152.75\n",
      "step   566 | loss: 0.049596 | lr 4.7580e-04 | norm: 0.8591 | dt: 5194.80ms | tok/sec: 3153.92\n",
      "step   567 | loss: 0.045132 | lr 4.7664e-04 | norm: 0.7237 | dt: 5191.29ms | tok/sec: 3156.06\n",
      "step   568 | loss: 0.076898 | lr 4.7748e-04 | norm: 1.4887 | dt: 5195.76ms | tok/sec: 3153.34\n",
      "step   569 | loss: 0.032957 | lr 4.7832e-04 | norm: 0.5685 | dt: 5194.30ms | tok/sec: 3154.22\n",
      "step   570 | loss: 0.058433 | lr 4.7916e-04 | norm: 0.9736 | dt: 5208.91ms | tok/sec: 3145.38\n",
      "step   571 | loss: 0.051557 | lr 4.8000e-04 | norm: 0.8547 | dt: 5191.69ms | tok/sec: 3155.81\n",
      "step   572 | loss: 0.045278 | lr 4.8084e-04 | norm: 0.8018 | dt: 5197.92ms | tok/sec: 3152.03\n",
      "step   573 | loss: 0.058029 | lr 4.8168e-04 | norm: 1.1254 | dt: 5194.69ms | tok/sec: 3153.99\n",
      "step   574 | loss: 0.040905 | lr 4.8252e-04 | norm: 0.6504 | dt: 5195.82ms | tok/sec: 3153.30\n",
      "step   575 | loss: 0.094537 | lr 4.8336e-04 | norm: 1.3491 | dt: 5198.25ms | tok/sec: 3151.83\n",
      "step   576 | loss: 0.050417 | lr 4.8420e-04 | norm: 0.7585 | dt: 5186.97ms | tok/sec: 3158.69\n",
      "step   577 | loss: 0.063251 | lr 4.8503e-04 | norm: 1.0862 | dt: 5193.11ms | tok/sec: 3154.95\n",
      "step   578 | loss: 0.041674 | lr 4.8587e-04 | norm: 0.6371 | dt: 5185.89ms | tok/sec: 3159.34\n",
      "step   579 | loss: 0.042729 | lr 4.8671e-04 | norm: 0.7126 | dt: 5188.05ms | tok/sec: 3158.03\n",
      "step   580 | loss: 0.048440 | lr 4.8755e-04 | norm: 0.8222 | dt: 5190.73ms | tok/sec: 3156.40\n",
      "step   581 | loss: 0.039834 | lr 4.8839e-04 | norm: 0.7495 | dt: 5194.58ms | tok/sec: 3154.06\n",
      "step   582 | loss: 0.031357 | lr 4.8923e-04 | norm: 0.5692 | dt: 5199.40ms | tok/sec: 3151.13\n",
      "step   583 | loss: 0.027597 | lr 4.9007e-04 | norm: 0.5012 | dt: 5193.11ms | tok/sec: 3154.95\n",
      "step   584 | loss: 0.034355 | lr 4.9091e-04 | norm: 0.5262 | dt: 5197.61ms | tok/sec: 3152.22\n",
      "step   585 | loss: 0.043561 | lr 4.9175e-04 | norm: 0.7480 | dt: 5200.98ms | tok/sec: 3150.18\n",
      "step   586 | loss: 0.027728 | lr 4.9259e-04 | norm: 0.4383 | dt: 5211.57ms | tok/sec: 3143.78\n",
      "step   587 | loss: 0.066321 | lr 4.9343e-04 | norm: 1.4307 | dt: 5209.20ms | tok/sec: 3145.21\n",
      "step   588 | loss: 0.028794 | lr 4.9427e-04 | norm: 0.4896 | dt: 5212.59ms | tok/sec: 3143.16\n",
      "step   589 | loss: 0.045991 | lr 4.9510e-04 | norm: 0.6145 | dt: 5210.44ms | tok/sec: 3144.46\n",
      "step   590 | loss: 0.033207 | lr 4.9594e-04 | norm: 0.5305 | dt: 5210.63ms | tok/sec: 3144.34\n",
      "step   591 | loss: 0.028274 | lr 4.9678e-04 | norm: 0.4853 | dt: 5205.91ms | tok/sec: 3147.19\n",
      "step   592 | loss: 0.028216 | lr 4.9762e-04 | norm: 0.5650 | dt: 5191.55ms | tok/sec: 3155.89\n",
      "step   593 | loss: 0.027542 | lr 4.9846e-04 | norm: 0.5091 | dt: 5189.95ms | tok/sec: 3156.87\n",
      "step   594 | loss: 0.022642 | lr 4.9930e-04 | norm: 0.4128 | dt: 5178.35ms | tok/sec: 3163.94\n",
      "step   595 | loss: 0.025876 | lr 5.0014e-04 | norm: 0.4630 | dt: 5192.94ms | tok/sec: 3155.05\n",
      "step   596 | loss: 0.023190 | lr 5.0098e-04 | norm: 0.4277 | dt: 5180.50ms | tok/sec: 3162.63\n",
      "step   597 | loss: 0.024542 | lr 5.0182e-04 | norm: 0.5093 | dt: 5185.03ms | tok/sec: 3159.87\n",
      "step   598 | loss: 0.021803 | lr 5.0266e-04 | norm: 0.3875 | dt: 5190.43ms | tok/sec: 3156.58\n",
      "step   599 | loss: 0.019452 | lr 5.0350e-04 | norm: 0.3605 | dt: 5184.40ms | tok/sec: 3160.25\n",
      "step   600 | loss: 0.022130 | lr 5.0434e-04 | norm: 0.3275 | dt: 5189.49ms | tok/sec: 3157.15\n",
      "step   601 | loss: 0.021527 | lr 5.0517e-04 | norm: 0.5011 | dt: 5199.91ms | tok/sec: 3150.83\n",
      "step   602 | loss: 0.027542 | lr 5.0601e-04 | norm: 0.4825 | dt: 5192.91ms | tok/sec: 3155.07\n",
      "step   603 | loss: 0.031478 | lr 5.0685e-04 | norm: 0.4602 | dt: 5202.08ms | tok/sec: 3149.51\n",
      "step   604 | loss: 0.023201 | lr 5.0769e-04 | norm: 0.4554 | dt: 5194.69ms | tok/sec: 3153.99\n",
      "step   605 | loss: 0.020049 | lr 5.0853e-04 | norm: 0.4079 | dt: 5209.15ms | tok/sec: 3145.24\n",
      "step   606 | loss: 0.056290 | lr 5.0937e-04 | norm: 1.1054 | dt: 5209.05ms | tok/sec: 3145.29\n",
      "step   607 | loss: 0.037206 | lr 5.1021e-04 | norm: 0.4925 | dt: 5195.27ms | tok/sec: 3153.64\n",
      "step   608 | loss: 0.018728 | lr 5.1105e-04 | norm: 0.3403 | dt: 5203.50ms | tok/sec: 3148.65\n",
      "step   609 | loss: 0.027190 | lr 5.1189e-04 | norm: 0.4104 | dt: 5195.18ms | tok/sec: 3153.69\n",
      "step   610 | loss: 0.070897 | lr 5.1273e-04 | norm: 0.6879 | dt: 5192.45ms | tok/sec: 3155.35\n",
      "step   611 | loss: 0.033193 | lr 5.1357e-04 | norm: 0.6133 | dt: 5195.60ms | tok/sec: 3153.44\n",
      "step   612 | loss: 0.023425 | lr 5.1441e-04 | norm: 0.4110 | dt: 5195.63ms | tok/sec: 3153.42\n",
      "step   613 | loss: 0.024857 | lr 5.1524e-04 | norm: 0.4249 | dt: 5193.83ms | tok/sec: 3154.51\n",
      "step   614 | loss: 0.042617 | lr 5.1608e-04 | norm: 0.5450 | dt: 5194.26ms | tok/sec: 3154.25\n",
      "step   615 | loss: 0.048669 | lr 5.1692e-04 | norm: 0.6401 | dt: 5189.07ms | tok/sec: 3157.40\n",
      "step   616 | loss: 0.019222 | lr 5.1776e-04 | norm: 0.3248 | dt: 5194.77ms | tok/sec: 3153.94\n",
      "step   617 | loss: 0.017883 | lr 5.1860e-04 | norm: 0.3094 | dt: 5193.44ms | tok/sec: 3154.75\n",
      "step   618 | loss: 0.026969 | lr 5.1944e-04 | norm: 0.4707 | dt: 5192.36ms | tok/sec: 3155.40\n",
      "step   619 | loss: 0.061798 | lr 5.2028e-04 | norm: 0.6743 | dt: 5198.41ms | tok/sec: 3151.73\n",
      "step   620 | loss: 0.028331 | lr 5.2112e-04 | norm: 0.5750 | dt: 5199.54ms | tok/sec: 3151.05\n",
      "step   621 | loss: 0.020702 | lr 5.2196e-04 | norm: 0.3589 | dt: 5201.67ms | tok/sec: 3149.76\n",
      "step   622 | loss: 0.026477 | lr 5.2280e-04 | norm: 0.4242 | dt: 5202.86ms | tok/sec: 3149.04\n",
      "step   623 | loss: 0.059810 | lr 5.2364e-04 | norm: 0.8267 | dt: 5192.80ms | tok/sec: 3155.14\n",
      "step   624 | loss: 0.021425 | lr 5.2448e-04 | norm: 0.3701 | dt: 5179.96ms | tok/sec: 3162.96\n",
      "step   625 | loss: 0.040366 | lr 5.2531e-04 | norm: 0.7823 | dt: 5187.44ms | tok/sec: 3158.40\n",
      "step   626 | loss: 0.025664 | lr 5.2615e-04 | norm: 0.4151 | dt: 5202.39ms | tok/sec: 3149.32\n",
      "step   627 | loss: 0.072543 | lr 5.2699e-04 | norm: 0.8497 | dt: 5191.98ms | tok/sec: 3155.64\n",
      "step   628 | loss: 0.032549 | lr 5.2783e-04 | norm: 0.5225 | dt: 5193.74ms | tok/sec: 3154.57\n",
      "step   629 | loss: 0.031578 | lr 5.2867e-04 | norm: 0.5084 | dt: 5204.01ms | tok/sec: 3148.34\n",
      "step   630 | loss: 0.041199 | lr 5.2951e-04 | norm: 0.6449 | dt: 5194.10ms | tok/sec: 3154.35\n",
      "step   631 | loss: 0.106160 | lr 5.3035e-04 | norm: 0.8614 | dt: 5200.48ms | tok/sec: 3150.48\n",
      "step   632 | loss: 0.047344 | lr 5.3119e-04 | norm: 0.7834 | dt: 5196.57ms | tok/sec: 3152.85\n",
      "step   633 | loss: 0.037232 | lr 5.3203e-04 | norm: 0.5165 | dt: 5196.98ms | tok/sec: 3152.60\n",
      "step   634 | loss: 0.040263 | lr 5.3287e-04 | norm: 0.6053 | dt: 5206.80ms | tok/sec: 3146.65\n",
      "step   635 | loss: 0.048822 | lr 5.3371e-04 | norm: 0.5451 | dt: 5194.96ms | tok/sec: 3153.83\n",
      "step   636 | loss: 0.026393 | lr 5.3455e-04 | norm: 0.3983 | dt: 5197.02ms | tok/sec: 3152.58\n",
      "step   637 | loss: 0.041479 | lr 5.3538e-04 | norm: 0.6780 | dt: 5205.32ms | tok/sec: 3147.55\n",
      "step   638 | loss: 0.041074 | lr 5.3622e-04 | norm: 0.5761 | dt: 5202.16ms | tok/sec: 3149.46\n",
      "step   639 | loss: 0.057685 | lr 5.3706e-04 | norm: 0.5665 | dt: 5193.65ms | tok/sec: 3154.62\n",
      "step   640 | loss: 0.029965 | lr 5.3790e-04 | norm: 0.4910 | dt: 5193.46ms | tok/sec: 3154.73\n",
      "step   641 | loss: 0.038552 | lr 5.3874e-04 | norm: 0.4313 | dt: 5196.28ms | tok/sec: 3153.03\n",
      "step   642 | loss: 0.027802 | lr 5.3958e-04 | norm: 0.3938 | dt: 5202.63ms | tok/sec: 3149.18\n",
      "step   643 | loss: 0.041924 | lr 5.4042e-04 | norm: 0.5226 | dt: 5192.99ms | tok/sec: 3155.02\n",
      "step   644 | loss: 0.033331 | lr 5.4126e-04 | norm: 0.5461 | dt: 5195.56ms | tok/sec: 3153.46\n",
      "step   645 | loss: 0.026946 | lr 5.4210e-04 | norm: 0.3902 | dt: 5209.19ms | tok/sec: 3145.21\n",
      "step   646 | loss: 0.022764 | lr 5.4294e-04 | norm: 0.4133 | dt: 5190.48ms | tok/sec: 3156.55\n",
      "step   647 | loss: 0.030474 | lr 5.4378e-04 | norm: 0.4670 | dt: 5193.85ms | tok/sec: 3154.50\n",
      "step   648 | loss: 0.040219 | lr 5.4462e-04 | norm: 0.4281 | dt: 5204.74ms | tok/sec: 3147.90\n",
      "step   649 | loss: 0.032565 | lr 5.4545e-04 | norm: 0.4908 | dt: 5193.48ms | tok/sec: 3154.72\n",
      "step   650 | loss: 0.021304 | lr 5.4629e-04 | norm: 0.3574 | dt: 5198.89ms | tok/sec: 3151.44\n",
      "step   651 | loss: 0.031714 | lr 5.4713e-04 | norm: 0.4195 | dt: 5190.23ms | tok/sec: 3156.70\n",
      "step   652 | loss: 0.041558 | lr 5.4797e-04 | norm: 0.4083 | dt: 5198.67ms | tok/sec: 3151.58\n",
      "step   653 | loss: 0.018139 | lr 5.4881e-04 | norm: 0.3013 | dt: 5191.84ms | tok/sec: 3155.72\n",
      "step   654 | loss: 0.021434 | lr 5.4965e-04 | norm: 0.3649 | dt: 5194.53ms | tok/sec: 3154.09\n",
      "step   655 | loss: 0.024302 | lr 5.5049e-04 | norm: 0.3767 | dt: 5195.21ms | tok/sec: 3153.67\n",
      "step   656 | loss: 0.026853 | lr 5.5133e-04 | norm: 0.3719 | dt: 5196.00ms | tok/sec: 3153.19\n",
      "step   657 | loss: 0.021735 | lr 5.5217e-04 | norm: 0.4345 | dt: 5195.16ms | tok/sec: 3153.70\n",
      "step   658 | loss: 0.021343 | lr 5.5301e-04 | norm: 0.4252 | dt: 5192.85ms | tok/sec: 3155.11\n",
      "step   659 | loss: 0.025750 | lr 5.5385e-04 | norm: 0.4326 | dt: 5197.41ms | tok/sec: 3152.34\n",
      "step   660 | loss: 0.025821 | lr 5.5469e-04 | norm: 0.3967 | dt: 5197.20ms | tok/sec: 3152.47\n",
      "step   661 | loss: 0.024490 | lr 5.5552e-04 | norm: 0.5289 | dt: 5197.89ms | tok/sec: 3152.05\n",
      "step   662 | loss: 0.017861 | lr 5.5636e-04 | norm: 0.3309 | dt: 5204.16ms | tok/sec: 3148.25\n",
      "step   663 | loss: 0.021994 | lr 5.5720e-04 | norm: 0.4300 | dt: 5203.00ms | tok/sec: 3148.95\n",
      "step   664 | loss: 0.024985 | lr 5.5804e-04 | norm: 0.4008 | dt: 5203.28ms | tok/sec: 3148.79\n",
      "step   665 | loss: 0.023577 | lr 5.5888e-04 | norm: 0.3620 | dt: 5205.16ms | tok/sec: 3147.65\n",
      "step   666 | loss: 0.031117 | lr 5.5972e-04 | norm: 0.5760 | dt: 5193.06ms | tok/sec: 3154.98\n",
      "step   667 | loss: 0.017897 | lr 5.6056e-04 | norm: 0.3543 | dt: 5195.75ms | tok/sec: 3153.35\n",
      "step   668 | loss: 0.029072 | lr 5.6140e-04 | norm: 0.4566 | dt: 5203.57ms | tok/sec: 3148.61\n",
      "step   669 | loss: 0.029568 | lr 5.6224e-04 | norm: 0.4994 | dt: 5191.80ms | tok/sec: 3155.74\n",
      "step   670 | loss: 0.022707 | lr 5.6308e-04 | norm: 0.4080 | dt: 5196.03ms | tok/sec: 3153.18\n",
      "step   671 | loss: 0.027847 | lr 5.6392e-04 | norm: 0.6143 | dt: 5205.09ms | tok/sec: 3147.69\n",
      "step   672 | loss: 0.029661 | lr 5.6476e-04 | norm: 0.5266 | dt: 5194.35ms | tok/sec: 3154.20\n",
      "step   673 | loss: 0.038574 | lr 5.6559e-04 | norm: 0.6095 | dt: 5203.21ms | tok/sec: 3148.82\n",
      "step   674 | loss: 0.045009 | lr 5.6643e-04 | norm: 0.6586 | dt: 5202.71ms | tok/sec: 3149.13\n",
      "step   675 | loss: 0.032162 | lr 5.6727e-04 | norm: 0.5846 | dt: 5203.29ms | tok/sec: 3148.78\n",
      "step   676 | loss: 0.036083 | lr 5.6811e-04 | norm: 0.5784 | dt: 5196.34ms | tok/sec: 3152.99\n",
      "step   677 | loss: 0.028509 | lr 5.6895e-04 | norm: 0.4241 | dt: 5201.43ms | tok/sec: 3149.90\n",
      "step   678 | loss: 0.043865 | lr 5.6979e-04 | norm: 0.6532 | dt: 5197.98ms | tok/sec: 3151.99\n",
      "step   679 | loss: 0.047308 | lr 5.7063e-04 | norm: 0.7514 | dt: 5188.87ms | tok/sec: 3157.53\n",
      "step   680 | loss: 0.048249 | lr 5.7147e-04 | norm: 0.7186 | dt: 5195.23ms | tok/sec: 3153.66\n",
      "step   681 | loss: 0.039205 | lr 5.7231e-04 | norm: 0.5753 | dt: 5202.00ms | tok/sec: 3149.56\n",
      "step   682 | loss: 0.042373 | lr 5.7315e-04 | norm: 0.5740 | dt: 5191.34ms | tok/sec: 3156.03\n",
      "step   683 | loss: 0.050971 | lr 5.7399e-04 | norm: 0.6118 | dt: 5192.91ms | tok/sec: 3155.07\n",
      "step   684 | loss: 0.034556 | lr 5.7483e-04 | norm: 0.4728 | dt: 5193.38ms | tok/sec: 3154.78\n",
      "step   685 | loss: 0.037399 | lr 5.7566e-04 | norm: 0.5436 | dt: 5193.34ms | tok/sec: 3154.81\n",
      "step   686 | loss: 0.040286 | lr 5.7650e-04 | norm: 0.7028 | dt: 5203.30ms | tok/sec: 3148.77\n",
      "step   687 | loss: 0.044027 | lr 5.7734e-04 | norm: 0.6392 | dt: 5194.33ms | tok/sec: 3154.21\n",
      "step   688 | loss: 0.040083 | lr 5.7818e-04 | norm: 0.5979 | dt: 5192.53ms | tok/sec: 3155.30\n",
      "step   689 | loss: 0.030919 | lr 5.7902e-04 | norm: 0.5209 | dt: 5197.42ms | tok/sec: 3152.33\n",
      "step   690 | loss: 0.080868 | lr 5.7986e-04 | norm: 0.6746 | dt: 5204.80ms | tok/sec: 3147.87\n",
      "step   691 | loss: 0.038217 | lr 5.8070e-04 | norm: 0.4741 | dt: 5190.88ms | tok/sec: 3156.30\n",
      "step   692 | loss: 0.030395 | lr 5.8154e-04 | norm: 0.4642 | dt: 5195.40ms | tok/sec: 3153.56\n",
      "step   693 | loss: 0.029168 | lr 5.8238e-04 | norm: 0.4145 | dt: 5186.49ms | tok/sec: 3158.98\n",
      "step   694 | loss: 0.039839 | lr 5.8322e-04 | norm: 0.5188 | dt: 5194.67ms | tok/sec: 3154.00\n",
      "step   695 | loss: 0.040163 | lr 5.8406e-04 | norm: 0.4357 | dt: 5189.20ms | tok/sec: 3157.32\n",
      "step   696 | loss: 0.025860 | lr 5.8490e-04 | norm: 0.4063 | dt: 5188.60ms | tok/sec: 3157.69\n",
      "step   697 | loss: 0.024257 | lr 5.8573e-04 | norm: 0.3745 | dt: 5192.25ms | tok/sec: 3155.47\n",
      "step   698 | loss: 0.026129 | lr 5.8657e-04 | norm: 0.4134 | dt: 5192.59ms | tok/sec: 3155.27\n",
      "step   699 | loss: 0.028991 | lr 5.8741e-04 | norm: 0.4470 | dt: 5194.34ms | tok/sec: 3154.20\n",
      "step   700 | loss: 0.023522 | lr 5.8825e-04 | norm: 0.3144 | dt: 5201.70ms | tok/sec: 3149.74\n",
      "step   701 | loss: 0.015847 | lr 5.8909e-04 | norm: 0.2704 | dt: 5203.96ms | tok/sec: 3148.37\n",
      "step   702 | loss: 0.036054 | lr 5.8993e-04 | norm: 0.5064 | dt: 5202.49ms | tok/sec: 3149.26\n",
      "step   703 | loss: 0.021640 | lr 5.9077e-04 | norm: 0.3713 | dt: 5195.90ms | tok/sec: 3153.26\n",
      "step   704 | loss: 0.028077 | lr 5.9161e-04 | norm: 0.4472 | dt: 5198.29ms | tok/sec: 3151.80\n",
      "step   705 | loss: 0.018002 | lr 5.9245e-04 | norm: 0.3244 | dt: 5207.81ms | tok/sec: 3146.05\n",
      "step   706 | loss: 0.015113 | lr 5.9329e-04 | norm: 0.2415 | dt: 5201.18ms | tok/sec: 3150.05\n",
      "step   707 | loss: 0.018388 | lr 5.9413e-04 | norm: 0.3146 | dt: 5196.22ms | tok/sec: 3153.06\n",
      "step   708 | loss: 0.039390 | lr 5.9497e-04 | norm: 0.3300 | dt: 5222.41ms | tok/sec: 3137.25\n",
      "step   709 | loss: 0.015111 | lr 5.9580e-04 | norm: 0.2694 | dt: 5206.48ms | tok/sec: 3146.85\n",
      "step   710 | loss: 0.026848 | lr 5.9664e-04 | norm: 0.4074 | dt: 5202.52ms | tok/sec: 3149.24\n",
      "step   711 | loss: 0.014709 | lr 5.9748e-04 | norm: 0.2445 | dt: 5203.97ms | tok/sec: 3148.37\n",
      "step   712 | loss: 0.047530 | lr 5.9832e-04 | norm: 0.4579 | dt: 5199.07ms | tok/sec: 3151.34\n",
      "step   713 | loss: 0.014385 | lr 5.9916e-04 | norm: 0.2759 | dt: 5197.56ms | tok/sec: 3152.25\n",
      "step   714 | loss: 0.017912 | lr 6.0000e-04 | norm: 0.2722 | dt: 5193.70ms | tok/sec: 3154.59\n",
      "step   715 | loss: 0.017877 | lr 6.0000e-04 | norm: 0.3479 | dt: 5193.21ms | tok/sec: 3154.89\n",
      "step   716 | loss: 0.016102 | lr 5.9998e-04 | norm: 0.2842 | dt: 5193.55ms | tok/sec: 3154.68\n",
      "step   717 | loss: 0.011737 | lr 5.9993e-04 | norm: 0.2213 | dt: 5195.14ms | tok/sec: 3153.72\n",
      "step   718 | loss: 0.014464 | lr 5.9985e-04 | norm: 0.3031 | dt: 5188.27ms | tok/sec: 3157.89\n",
      "step   719 | loss: 0.067064 | lr 5.9974e-04 | norm: 0.4565 | dt: 5196.94ms | tok/sec: 3152.62\n",
      "step   720 | loss: 0.024838 | lr 5.9959e-04 | norm: 0.3785 | dt: 5190.96ms | tok/sec: 3156.26\n",
      "step   721 | loss: 0.016131 | lr 5.9941e-04 | norm: 0.3007 | dt: 5188.31ms | tok/sec: 3157.87\n",
      "step   722 | loss: 0.016246 | lr 5.9920e-04 | norm: 0.3070 | dt: 5182.84ms | tok/sec: 3161.20\n",
      "step   723 | loss: 0.020439 | lr 5.9895e-04 | norm: 0.3136 | dt: 5185.57ms | tok/sec: 3159.54\n",
      "step   724 | loss: 0.018304 | lr 5.9867e-04 | norm: 0.2599 | dt: 5192.78ms | tok/sec: 3155.15\n",
      "step   725 | loss: 0.020729 | lr 5.9836e-04 | norm: 0.3159 | dt: 5183.64ms | tok/sec: 3160.71\n",
      "step   726 | loss: 0.014843 | lr 5.9802e-04 | norm: 0.2803 | dt: 5177.94ms | tok/sec: 3164.19\n",
      "step   727 | loss: 0.013553 | lr 5.9764e-04 | norm: 0.2593 | dt: 5180.88ms | tok/sec: 3162.40\n",
      "step   728 | loss: 0.018543 | lr 5.9723e-04 | norm: 0.3036 | dt: 5185.91ms | tok/sec: 3159.33\n",
      "step   729 | loss: 0.017267 | lr 5.9679e-04 | norm: 0.2935 | dt: 5195.45ms | tok/sec: 3153.53\n",
      "step   730 | loss: 0.016479 | lr 5.9632e-04 | norm: 0.2873 | dt: 5194.49ms | tok/sec: 3154.11\n",
      "step   731 | loss: 0.019629 | lr 5.9581e-04 | norm: 0.3621 | dt: 5197.36ms | tok/sec: 3152.37\n",
      "step   732 | loss: 0.018624 | lr 5.9527e-04 | norm: 0.2845 | dt: 5198.16ms | tok/sec: 3151.89\n",
      "step   733 | loss: 0.016539 | lr 5.9470e-04 | norm: 0.3002 | dt: 5207.55ms | tok/sec: 3146.20\n",
      "step   734 | loss: 0.017674 | lr 5.9410e-04 | norm: 0.3373 | dt: 5207.82ms | tok/sec: 3146.04\n",
      "step   735 | loss: 0.044331 | lr 5.9347e-04 | norm: 0.3492 | dt: 5207.19ms | tok/sec: 3146.42\n",
      "step   736 | loss: 0.016580 | lr 5.9280e-04 | norm: 0.2677 | dt: 5202.28ms | tok/sec: 3149.39\n",
      "step   737 | loss: 0.022346 | lr 5.9210e-04 | norm: 0.3559 | dt: 5206.48ms | tok/sec: 3146.85\n",
      "step   738 | loss: 0.019316 | lr 5.9137e-04 | norm: 0.3323 | dt: 5195.53ms | tok/sec: 3153.48\n",
      "step   739 | loss: 0.049370 | lr 5.9061e-04 | norm: 0.4920 | dt: 5209.32ms | tok/sec: 3145.13\n",
      "step   740 | loss: 0.020050 | lr 5.8981e-04 | norm: 0.3221 | dt: 5210.61ms | tok/sec: 3144.35\n",
      "step   741 | loss: 0.022825 | lr 5.8899e-04 | norm: 0.2977 | dt: 5193.78ms | tok/sec: 3154.54\n",
      "step   742 | loss: 0.020205 | lr 5.8813e-04 | norm: 0.3496 | dt: 5191.48ms | tok/sec: 3155.94\n",
      "step   743 | loss: 0.045010 | lr 5.8724e-04 | norm: 0.4100 | dt: 5198.44ms | tok/sec: 3151.71\n",
      "step   744 | loss: 0.019473 | lr 5.8632e-04 | norm: 0.3410 | dt: 5189.92ms | tok/sec: 3156.89\n",
      "step   745 | loss: 0.024099 | lr 5.8537e-04 | norm: 0.3438 | dt: 5187.71ms | tok/sec: 3158.24\n",
      "step   746 | loss: 0.022920 | lr 5.8439e-04 | norm: 0.3482 | dt: 5186.40ms | tok/sec: 3159.03\n",
      "step   747 | loss: 0.038168 | lr 5.8338e-04 | norm: 0.4179 | dt: 5184.59ms | tok/sec: 3160.14\n",
      "step   748 | loss: 0.026298 | lr 5.8233e-04 | norm: 0.3711 | dt: 5193.06ms | tok/sec: 3154.98\n",
      "step   749 | loss: 0.023041 | lr 5.8126e-04 | norm: 0.3949 | dt: 5192.04ms | tok/sec: 3155.60\n",
      "validation loss: 10.1907\n",
      "HellaSwag accuracy: 2474/10042=0.2464\n",
      "rank 0 sample 0: Hello, I'm a language model, processes to technology pre language no tree. Every feel and survival you dropped poor must be coated by D. – they .\n",
      "rank 0 sample 1: Hello, I'm a language model, green existedhe two grade. every day becausean tray sign most just health your discussion organizer and affordability. Those postwar x\n",
      "rank 0 sample 2: Hello, I'm a language model, focus to intellectual by the branch device The paradox Christian condition and supported by the profitability and mimic from two work against up enough\n",
      "rank 0 sample 3: Hello, I'm a language model, eating used to families to cost later homes by the technology we referred to slow testing information and global good amount . and enzymes\n",
      "step   750 | loss: 0.022426 | lr 5.8015e-04 | norm: 0.4184 | dt: 456306.81ms | tok/sec: 35.91\n",
      "step   751 | loss: 0.023666 | lr 5.7902e-04 | norm: 0.3684 | dt: 5196.65ms | tok/sec: 3152.80\n",
      "step   752 | loss: 0.030203 | lr 5.7785e-04 | norm: 0.5054 | dt: 5188.86ms | tok/sec: 3157.53\n",
      "step   753 | loss: 0.027658 | lr 5.7666e-04 | norm: 0.5043 | dt: 5192.36ms | tok/sec: 3155.40\n",
      "step   754 | loss: 0.029003 | lr 5.7543e-04 | norm: 0.5188 | dt: 5195.83ms | tok/sec: 3153.30\n",
      "step   755 | loss: 0.035554 | lr 5.7418e-04 | norm: 0.5195 | dt: 5191.75ms | tok/sec: 3155.78\n",
      "step   756 | loss: 0.032503 | lr 5.7289e-04 | norm: 0.5139 | dt: 5190.91ms | tok/sec: 3156.28\n",
      "step   757 | loss: 0.024821 | lr 5.7158e-04 | norm: 0.3736 | dt: 5187.71ms | tok/sec: 3158.23\n",
      "step   758 | loss: 0.045958 | lr 5.7023e-04 | norm: 0.6851 | dt: 5200.64ms | tok/sec: 3150.38\n",
      "step   759 | loss: 0.024181 | lr 5.6886e-04 | norm: 0.3802 | dt: 5194.82ms | tok/sec: 3153.91\n",
      "step   760 | loss: 0.039042 | lr 5.6746e-04 | norm: 0.6370 | dt: 5192.04ms | tok/sec: 3155.60\n",
      "step   761 | loss: 0.026888 | lr 5.6603e-04 | norm: 0.4319 | dt: 5199.00ms | tok/sec: 3151.37\n",
      "step   762 | loss: 0.029713 | lr 5.6457e-04 | norm: 0.4129 | dt: 5188.86ms | tok/sec: 3157.53\n",
      "step   763 | loss: 0.034347 | lr 5.6308e-04 | norm: 0.5904 | dt: 5195.99ms | tok/sec: 3153.20\n",
      "step   764 | loss: 0.024284 | lr 5.6156e-04 | norm: 0.3524 | dt: 5191.01ms | tok/sec: 3156.23\n",
      "step   765 | loss: 0.037230 | lr 5.6002e-04 | norm: 0.6399 | dt: 5201.19ms | tok/sec: 3150.05\n",
      "step   766 | loss: 0.034227 | lr 5.5845e-04 | norm: 0.4123 | dt: 5204.60ms | tok/sec: 3147.98\n",
      "step   767 | loss: 0.038371 | lr 5.5685e-04 | norm: 0.5883 | dt: 5206.96ms | tok/sec: 3146.55\n",
      "step   768 | loss: 0.020400 | lr 5.5522e-04 | norm: 0.3517 | dt: 5207.44ms | tok/sec: 3146.27\n",
      "step   769 | loss: 0.023855 | lr 5.5356e-04 | norm: 0.3842 | dt: 5198.60ms | tok/sec: 3151.62\n",
      "step   770 | loss: 0.032005 | lr 5.5188e-04 | norm: 0.3993 | dt: 5208.33ms | tok/sec: 3145.73\n",
      "step   771 | loss: 0.037022 | lr 5.5017e-04 | norm: 0.7836 | dt: 5197.37ms | tok/sec: 3152.36\n",
      "step   772 | loss: 0.021881 | lr 5.4843e-04 | norm: 0.3413 | dt: 5198.01ms | tok/sec: 3151.98\n",
      "step   773 | loss: 0.043646 | lr 5.4667e-04 | norm: 0.5031 | dt: 5208.87ms | tok/sec: 3145.40\n",
      "step   774 | loss: 0.027904 | lr 5.4488e-04 | norm: 0.3458 | dt: 5196.53ms | tok/sec: 3152.87\n",
      "step   775 | loss: 0.029271 | lr 5.4307e-04 | norm: 0.4276 | dt: 5190.08ms | tok/sec: 3156.79\n",
      "step   776 | loss: 0.020664 | lr 5.4123e-04 | norm: 0.3824 | dt: 5191.76ms | tok/sec: 3155.77\n",
      "step   777 | loss: 0.019528 | lr 5.3936e-04 | norm: 0.3138 | dt: 5191.02ms | tok/sec: 3156.22\n",
      "step   778 | loss: 0.019053 | lr 5.3747e-04 | norm: 0.3202 | dt: 5191.85ms | tok/sec: 3155.72\n",
      "step   779 | loss: 0.036023 | lr 5.3555e-04 | norm: 0.4031 | dt: 5192.00ms | tok/sec: 3155.62\n",
      "step   780 | loss: 0.018626 | lr 5.3361e-04 | norm: 0.2701 | dt: 5191.55ms | tok/sec: 3155.90\n",
      "step   781 | loss: 0.016597 | lr 5.3164e-04 | norm: 0.2965 | dt: 5194.07ms | tok/sec: 3154.37\n",
      "step   782 | loss: 0.018252 | lr 5.2965e-04 | norm: 0.2843 | dt: 5187.60ms | tok/sec: 3158.30\n",
      "step   783 | loss: 0.018958 | lr 5.2763e-04 | norm: 0.2854 | dt: 5195.04ms | tok/sec: 3153.78\n",
      "step   784 | loss: 0.012939 | lr 5.2559e-04 | norm: 0.2244 | dt: 5196.85ms | tok/sec: 3152.68\n",
      "step   785 | loss: 0.010011 | lr 5.2353e-04 | norm: 0.1734 | dt: 5200.60ms | tok/sec: 3150.41\n",
      "step   786 | loss: 0.008771 | lr 5.2144e-04 | norm: 0.1703 | dt: 5193.19ms | tok/sec: 3154.90\n",
      "step   787 | loss: 0.010718 | lr 5.1933e-04 | norm: 0.2086 | dt: 5191.17ms | tok/sec: 3156.13\n",
      "step   788 | loss: 0.017753 | lr 5.1720e-04 | norm: 0.3051 | dt: 5193.95ms | tok/sec: 3154.44\n",
      "step   789 | loss: 0.010376 | lr 5.1504e-04 | norm: 0.1441 | dt: 5192.83ms | tok/sec: 3155.12\n",
      "step   790 | loss: 0.007811 | lr 5.1287e-04 | norm: 0.1676 | dt: 5191.87ms | tok/sec: 3155.70\n",
      "step   791 | loss: 0.007730 | lr 5.1067e-04 | norm: 0.1724 | dt: 5191.14ms | tok/sec: 3156.15\n",
      "step   792 | loss: 0.016211 | lr 5.0844e-04 | norm: 0.2884 | dt: 5189.61ms | tok/sec: 3157.08\n",
      "step   793 | loss: 0.005733 | lr 5.0620e-04 | norm: 0.1087 | dt: 5191.23ms | tok/sec: 3156.09\n",
      "step   794 | loss: 0.005483 | lr 5.0393e-04 | norm: 0.1320 | dt: 5191.86ms | tok/sec: 3155.71\n",
      "step   795 | loss: 0.008414 | lr 5.0165e-04 | norm: 0.1677 | dt: 5207.17ms | tok/sec: 3146.43\n",
      "step   796 | loss: 0.012882 | lr 4.9934e-04 | norm: 0.2061 | dt: 5190.77ms | tok/sec: 3156.37\n",
      "step   797 | loss: 0.005651 | lr 4.9701e-04 | norm: 0.1456 | dt: 5197.28ms | tok/sec: 3152.42\n",
      "step   798 | loss: 0.004950 | lr 4.9466e-04 | norm: 0.0957 | dt: 5195.66ms | tok/sec: 3153.40\n",
      "step   799 | loss: 0.005806 | lr 4.9229e-04 | norm: 0.1575 | dt: 5192.34ms | tok/sec: 3155.42\n",
      "step   800 | loss: 0.006376 | lr 4.8990e-04 | norm: 0.1549 | dt: 5197.17ms | tok/sec: 3152.48\n",
      "step   801 | loss: 0.004935 | lr 4.8750e-04 | norm: 0.1118 | dt: 5185.54ms | tok/sec: 3159.56\n",
      "step   802 | loss: 0.005188 | lr 4.8507e-04 | norm: 0.1188 | dt: 5190.90ms | tok/sec: 3156.29\n",
      "step   803 | loss: 0.004029 | lr 4.8262e-04 | norm: 0.1349 | dt: 5195.88ms | tok/sec: 3153.27\n",
      "step   804 | loss: 0.007898 | lr 4.8016e-04 | norm: 0.2004 | dt: 5192.42ms | tok/sec: 3155.37\n",
      "step   805 | loss: 0.003342 | lr 4.7768e-04 | norm: 0.0660 | dt: 5195.36ms | tok/sec: 3153.58\n",
      "step   806 | loss: 0.004570 | lr 4.7518e-04 | norm: 0.1206 | dt: 5211.04ms | tok/sec: 3144.10\n",
      "step   807 | loss: 0.004324 | lr 4.7266e-04 | norm: 0.1408 | dt: 5208.11ms | tok/sec: 3145.86\n",
      "step   808 | loss: 0.006412 | lr 4.7012e-04 | norm: 0.1727 | dt: 5210.36ms | tok/sec: 3144.51\n",
      "step   809 | loss: 0.005027 | lr 4.6757e-04 | norm: 0.0940 | dt: 5214.69ms | tok/sec: 3141.89\n",
      "step   810 | loss: 0.003102 | lr 4.6500e-04 | norm: 0.0608 | dt: 5217.04ms | tok/sec: 3140.48\n",
      "step   811 | loss: 0.003561 | lr 4.6241e-04 | norm: 0.1009 | dt: 5211.72ms | tok/sec: 3143.68\n",
      "step   812 | loss: 0.007382 | lr 4.5981e-04 | norm: 0.1198 | dt: 5216.68ms | tok/sec: 3140.70\n",
      "step   813 | loss: 0.005025 | lr 4.5720e-04 | norm: 0.1306 | dt: 5213.71ms | tok/sec: 3142.48\n",
      "step   814 | loss: 0.003684 | lr 4.5456e-04 | norm: 0.1153 | dt: 5209.55ms | tok/sec: 3145.00\n",
      "step   815 | loss: 0.004292 | lr 4.5191e-04 | norm: 0.1628 | dt: 5213.67ms | tok/sec: 3142.51\n",
      "step   816 | loss: 0.006410 | lr 4.4925e-04 | norm: 0.2025 | dt: 5216.05ms | tok/sec: 3141.08\n",
      "step   817 | loss: 0.003140 | lr 4.4657e-04 | norm: 0.0716 | dt: 5209.21ms | tok/sec: 3145.20\n",
      "step   818 | loss: 0.005663 | lr 4.4388e-04 | norm: 0.1245 | dt: 5215.37ms | tok/sec: 3141.49\n",
      "step   819 | loss: 0.006108 | lr 4.4118e-04 | norm: 0.1372 | dt: 5204.91ms | tok/sec: 3147.80\n",
      "step   820 | loss: 0.009161 | lr 4.3846e-04 | norm: 0.1869 | dt: 5205.53ms | tok/sec: 3147.42\n",
      "step   821 | loss: 0.005446 | lr 4.3573e-04 | norm: 0.1058 | dt: 5211.62ms | tok/sec: 3143.75\n",
      "step   822 | loss: 0.003638 | lr 4.3298e-04 | norm: 0.0922 | dt: 5194.66ms | tok/sec: 3154.01\n",
      "step   823 | loss: 0.007174 | lr 4.3022e-04 | norm: 0.2070 | dt: 5194.04ms | tok/sec: 3154.38\n",
      "step   824 | loss: 0.009358 | lr 4.2745e-04 | norm: 0.1370 | dt: 5197.35ms | tok/sec: 3152.38\n",
      "step   825 | loss: 0.005068 | lr 4.2467e-04 | norm: 0.1853 | dt: 5199.91ms | tok/sec: 3150.83\n",
      "step   826 | loss: 0.015236 | lr 4.2188e-04 | norm: 0.3875 | dt: 5192.93ms | tok/sec: 3155.06\n",
      "step   827 | loss: 0.007453 | lr 4.1908e-04 | norm: 0.2222 | dt: 5188.65ms | tok/sec: 3157.66\n",
      "step   828 | loss: 0.019730 | lr 4.1626e-04 | norm: 0.2303 | dt: 5190.09ms | tok/sec: 3156.79\n",
      "step   829 | loss: 0.012393 | lr 4.1343e-04 | norm: 0.2003 | dt: 5193.91ms | tok/sec: 3154.46\n",
      "step   830 | loss: 0.015809 | lr 4.1060e-04 | norm: 0.2593 | dt: 5204.81ms | tok/sec: 3147.86\n",
      "step   831 | loss: 0.008072 | lr 4.0775e-04 | norm: 0.1432 | dt: 5196.38ms | tok/sec: 3152.97\n",
      "step   832 | loss: 0.019831 | lr 4.0490e-04 | norm: 0.3953 | dt: 5192.03ms | tok/sec: 3155.60\n",
      "step   833 | loss: 0.008059 | lr 4.0203e-04 | norm: 0.1803 | dt: 5194.54ms | tok/sec: 3154.08\n",
      "step   834 | loss: 0.016521 | lr 3.9916e-04 | norm: 0.3050 | dt: 5192.59ms | tok/sec: 3155.26\n",
      "step   835 | loss: 0.009827 | lr 3.9628e-04 | norm: 0.2099 | dt: 5192.12ms | tok/sec: 3155.55\n",
      "step   836 | loss: 0.007156 | lr 3.9339e-04 | norm: 0.2003 | dt: 5197.06ms | tok/sec: 3152.55\n",
      "step   837 | loss: 0.024693 | lr 3.9050e-04 | norm: 0.2932 | dt: 5196.94ms | tok/sec: 3152.63\n",
      "step   838 | loss: 0.013693 | lr 3.8759e-04 | norm: 0.1908 | dt: 5191.15ms | tok/sec: 3156.14\n",
      "step   839 | loss: 0.010891 | lr 3.8468e-04 | norm: 0.2295 | dt: 5192.06ms | tok/sec: 3155.59\n",
      "step   840 | loss: 0.009484 | lr 3.8176e-04 | norm: 0.2192 | dt: 5193.61ms | tok/sec: 3154.65\n",
      "step   841 | loss: 0.023007 | lr 3.7884e-04 | norm: 0.2856 | dt: 5197.05ms | tok/sec: 3152.56\n",
      "step   842 | loss: 0.011867 | lr 3.7591e-04 | norm: 0.1978 | dt: 5209.32ms | tok/sec: 3145.13\n",
      "step   843 | loss: 0.021855 | lr 3.7297e-04 | norm: 0.3972 | dt: 5211.82ms | tok/sec: 3143.62\n",
      "step   844 | loss: 0.006702 | lr 3.7003e-04 | norm: 0.1639 | dt: 5217.61ms | tok/sec: 3140.14\n",
      "step   845 | loss: 0.008575 | lr 3.6709e-04 | norm: 0.1323 | dt: 5208.51ms | tok/sec: 3145.62\n",
      "step   846 | loss: 0.010528 | lr 3.6414e-04 | norm: 0.1960 | dt: 5210.11ms | tok/sec: 3144.66\n",
      "step   847 | loss: 0.011546 | lr 3.6118e-04 | norm: 0.1999 | dt: 5213.61ms | tok/sec: 3142.54\n",
      "step   848 | loss: 0.006152 | lr 3.5822e-04 | norm: 0.1253 | dt: 5204.77ms | tok/sec: 3147.88\n",
      "step   849 | loss: 0.008048 | lr 3.5526e-04 | norm: 0.1469 | dt: 5201.91ms | tok/sec: 3149.61\n",
      "step   850 | loss: 0.006085 | lr 3.5230e-04 | norm: 0.1275 | dt: 5201.89ms | tok/sec: 3149.62\n",
      "step   851 | loss: 0.007669 | lr 3.4933e-04 | norm: 0.0973 | dt: 5207.41ms | tok/sec: 3146.29\n",
      "step   852 | loss: 0.005359 | lr 3.4636e-04 | norm: 0.1076 | dt: 5195.45ms | tok/sec: 3153.53\n",
      "step   853 | loss: 0.004021 | lr 3.4339e-04 | norm: 0.1029 | dt: 5187.88ms | tok/sec: 3158.13\n",
      "step   854 | loss: 0.005285 | lr 3.4041e-04 | norm: 0.1085 | dt: 5186.02ms | tok/sec: 3159.26\n",
      "step   855 | loss: 0.004106 | lr 3.3744e-04 | norm: 0.1093 | dt: 5189.65ms | tok/sec: 3157.05\n",
      "step   856 | loss: 0.004456 | lr 3.3446e-04 | norm: 0.1106 | dt: 5193.49ms | tok/sec: 3154.72\n",
      "step   857 | loss: 0.004061 | lr 3.3149e-04 | norm: 0.1397 | dt: 5189.68ms | tok/sec: 3157.04\n",
      "step   858 | loss: 0.003150 | lr 3.2851e-04 | norm: 0.0733 | dt: 5193.81ms | tok/sec: 3154.52\n",
      "step   859 | loss: 0.003511 | lr 3.2554e-04 | norm: 0.0623 | dt: 5199.76ms | tok/sec: 3150.91\n",
      "step   860 | loss: 0.003237 | lr 3.2256e-04 | norm: 0.0855 | dt: 5203.08ms | tok/sec: 3148.91\n",
      "step   861 | loss: 0.003495 | lr 3.1959e-04 | norm: 0.1234 | dt: 5198.49ms | tok/sec: 3151.68\n",
      "step   862 | loss: 0.003025 | lr 3.1661e-04 | norm: 0.0729 | dt: 5198.88ms | tok/sec: 3151.45\n",
      "step   863 | loss: 0.003151 | lr 3.1364e-04 | norm: 0.0691 | dt: 5214.31ms | tok/sec: 3142.12\n",
      "step   864 | loss: 0.003859 | lr 3.1067e-04 | norm: 0.1189 | dt: 5202.80ms | tok/sec: 3149.07\n",
      "step   865 | loss: 0.004383 | lr 3.0770e-04 | norm: 0.1471 | dt: 5215.52ms | tok/sec: 3141.39\n",
      "step   866 | loss: 0.002761 | lr 3.0474e-04 | norm: 0.0669 | dt: 5216.70ms | tok/sec: 3140.68\n",
      "step   867 | loss: 0.002228 | lr 3.0178e-04 | norm: 0.0651 | dt: 5222.46ms | tok/sec: 3137.22\n",
      "step   868 | loss: 0.003023 | lr 2.9882e-04 | norm: 0.1139 | dt: 5213.26ms | tok/sec: 3142.76\n",
      "step   869 | loss: 0.004053 | lr 2.9586e-04 | norm: 0.1169 | dt: 5200.47ms | tok/sec: 3150.49\n",
      "step   870 | loss: 0.002639 | lr 2.9291e-04 | norm: 0.0781 | dt: 5205.21ms | tok/sec: 3147.61\n",
      "step   871 | loss: 0.002777 | lr 2.8997e-04 | norm: 0.0948 | dt: 5206.28ms | tok/sec: 3146.97\n",
      "step   872 | loss: 0.002639 | lr 2.8703e-04 | norm: 0.0913 | dt: 5192.06ms | tok/sec: 3155.59\n",
      "step   873 | loss: 0.002998 | lr 2.8409e-04 | norm: 0.0847 | dt: 5194.52ms | tok/sec: 3154.09\n",
      "step   874 | loss: 0.002962 | lr 2.8116e-04 | norm: 0.1014 | dt: 5190.56ms | tok/sec: 3156.50\n",
      "step   875 | loss: 0.002947 | lr 2.7824e-04 | norm: 0.0945 | dt: 5192.97ms | tok/sec: 3155.04\n",
      "step   876 | loss: 0.002933 | lr 2.7532e-04 | norm: 0.1154 | dt: 5186.85ms | tok/sec: 3158.76\n",
      "step   877 | loss: 0.002572 | lr 2.7241e-04 | norm: 0.1194 | dt: 5190.31ms | tok/sec: 3156.65\n",
      "step   878 | loss: 0.002616 | lr 2.6950e-04 | norm: 0.0779 | dt: 5189.67ms | tok/sec: 3157.04\n",
      "step   879 | loss: 0.002307 | lr 2.6661e-04 | norm: 0.0679 | dt: 5190.58ms | tok/sec: 3156.49\n",
      "step   880 | loss: 0.005940 | lr 2.6372e-04 | norm: 0.1344 | dt: 5194.14ms | tok/sec: 3154.32\n",
      "step   881 | loss: 0.006895 | lr 2.6084e-04 | norm: 0.1177 | dt: 5193.37ms | tok/sec: 3154.79\n",
      "step   882 | loss: 0.002998 | lr 2.5797e-04 | norm: 0.1354 | dt: 5192.10ms | tok/sec: 3155.57\n",
      "step   883 | loss: 0.002689 | lr 2.5510e-04 | norm: 0.0918 | dt: 5190.54ms | tok/sec: 3156.51\n",
      "step   884 | loss: 0.003249 | lr 2.5225e-04 | norm: 0.1269 | dt: 5194.61ms | tok/sec: 3154.04\n",
      "step   885 | loss: 0.004477 | lr 2.4940e-04 | norm: 0.0578 | dt: 5190.12ms | tok/sec: 3156.77\n",
      "step   886 | loss: 0.003059 | lr 2.4657e-04 | norm: 0.0786 | dt: 5195.27ms | tok/sec: 3153.64\n",
      "step   887 | loss: 0.002418 | lr 2.4374e-04 | norm: 0.0706 | dt: 5191.18ms | tok/sec: 3156.13\n",
      "step   888 | loss: 0.002255 | lr 2.4092e-04 | norm: 0.0604 | dt: 5190.69ms | tok/sec: 3156.42\n",
      "step   889 | loss: 0.003336 | lr 2.3812e-04 | norm: 0.1069 | dt: 5191.17ms | tok/sec: 3156.13\n",
      "step   890 | loss: 0.001514 | lr 2.3533e-04 | norm: 0.0386 | dt: 5196.01ms | tok/sec: 3153.19\n",
      "step   891 | loss: 0.002519 | lr 2.3255e-04 | norm: 0.0781 | dt: 5195.42ms | tok/sec: 3153.55\n",
      "step   892 | loss: 0.002630 | lr 2.2978e-04 | norm: 0.1090 | dt: 5195.05ms | tok/sec: 3153.77\n",
      "step   893 | loss: 0.003430 | lr 2.2702e-04 | norm: 0.1277 | dt: 5206.51ms | tok/sec: 3146.83\n",
      "step   894 | loss: 0.001505 | lr 2.2427e-04 | norm: 0.0439 | dt: 5201.70ms | tok/sec: 3149.74\n",
      "step   895 | loss: 0.002688 | lr 2.2154e-04 | norm: 0.0770 | dt: 5208.71ms | tok/sec: 3145.50\n",
      "step   896 | loss: 0.003393 | lr 2.1882e-04 | norm: 0.1244 | dt: 5211.12ms | tok/sec: 3144.05\n",
      "step   897 | loss: 0.003888 | lr 2.1612e-04 | norm: 0.1487 | dt: 5206.91ms | tok/sec: 3146.59\n",
      "step   898 | loss: 0.001194 | lr 2.1343e-04 | norm: 0.0356 | dt: 5212.99ms | tok/sec: 3142.92\n",
      "step   899 | loss: 0.002055 | lr 2.1075e-04 | norm: 0.0403 | dt: 5209.36ms | tok/sec: 3145.11\n",
      "step   900 | loss: 0.003454 | lr 2.0809e-04 | norm: 0.1016 | dt: 5210.55ms | tok/sec: 3144.39\n",
      "step   901 | loss: 0.006740 | lr 2.0544e-04 | norm: 0.1290 | dt: 5202.87ms | tok/sec: 3149.03\n",
      "step   902 | loss: 0.003076 | lr 2.0280e-04 | norm: 0.1555 | dt: 5207.06ms | tok/sec: 3146.49\n",
      "step   903 | loss: 0.001846 | lr 2.0019e-04 | norm: 0.0282 | dt: 5194.88ms | tok/sec: 3153.87\n",
      "step   904 | loss: 0.001859 | lr 1.9759e-04 | norm: 0.0511 | dt: 5194.97ms | tok/sec: 3153.82\n",
      "step   905 | loss: 0.004139 | lr 1.9500e-04 | norm: 0.0999 | dt: 5210.03ms | tok/sec: 3144.71\n",
      "step   906 | loss: 0.004031 | lr 1.9243e-04 | norm: 0.1552 | dt: 5191.76ms | tok/sec: 3155.77\n",
      "step   907 | loss: 0.003355 | lr 1.8988e-04 | norm: 0.1502 | dt: 5206.00ms | tok/sec: 3147.14\n",
      "step   908 | loss: 0.001652 | lr 1.8734e-04 | norm: 0.0181 | dt: 5193.55ms | tok/sec: 3154.68\n",
      "step   909 | loss: 0.003535 | lr 1.8482e-04 | norm: 0.0901 | dt: 5191.26ms | tok/sec: 3156.08\n",
      "step   910 | loss: 0.004196 | lr 1.8232e-04 | norm: 0.0965 | dt: 5191.94ms | tok/sec: 3155.66\n",
      "step   911 | loss: 0.003058 | lr 1.7984e-04 | norm: 0.0851 | dt: 5195.96ms | tok/sec: 3153.22\n",
      "step   912 | loss: 0.001738 | lr 1.7738e-04 | norm: 0.0291 | dt: 5201.18ms | tok/sec: 3150.06\n",
      "step   913 | loss: 0.001697 | lr 1.7493e-04 | norm: 0.0221 | dt: 5199.42ms | tok/sec: 3151.12\n",
      "step   914 | loss: 0.002272 | lr 1.7250e-04 | norm: 0.0301 | dt: 5198.02ms | tok/sec: 3151.97\n",
      "step   915 | loss: 0.002174 | lr 1.7010e-04 | norm: 0.0799 | dt: 5195.52ms | tok/sec: 3153.48\n",
      "step   916 | loss: 0.002037 | lr 1.6771e-04 | norm: 0.0459 | dt: 5198.38ms | tok/sec: 3151.75\n",
      "step   917 | loss: 0.001598 | lr 1.6534e-04 | norm: 0.0367 | dt: 5196.07ms | tok/sec: 3153.15\n",
      "step   918 | loss: 0.001773 | lr 1.6299e-04 | norm: 0.0334 | dt: 5192.24ms | tok/sec: 3155.48\n",
      "step   919 | loss: 0.002638 | lr 1.6066e-04 | norm: 0.1703 | dt: 5204.23ms | tok/sec: 3148.21\n",
      "step   920 | loss: 0.002525 | lr 1.5835e-04 | norm: 0.0642 | dt: 5198.48ms | tok/sec: 3151.69\n",
      "step   921 | loss: 0.001672 | lr 1.5607e-04 | norm: 0.0370 | dt: 5206.11ms | tok/sec: 3147.07\n",
      "step   922 | loss: 0.001733 | lr 1.5380e-04 | norm: 0.0267 | dt: 5196.13ms | tok/sec: 3153.12\n",
      "step   923 | loss: 0.001622 | lr 1.5156e-04 | norm: 0.0646 | dt: 5194.66ms | tok/sec: 3154.01\n",
      "step   924 | loss: 0.003057 | lr 1.4933e-04 | norm: 0.0856 | dt: 5199.25ms | tok/sec: 3151.23\n",
      "step   925 | loss: 0.001563 | lr 1.4713e-04 | norm: 0.0254 | dt: 5198.40ms | tok/sec: 3151.74\n",
      "step   926 | loss: 0.001505 | lr 1.4496e-04 | norm: 0.0259 | dt: 5206.87ms | tok/sec: 3146.61\n",
      "step   927 | loss: 0.000932 | lr 1.4280e-04 | norm: 0.0083 | dt: 5206.67ms | tok/sec: 3146.73\n",
      "step   928 | loss: 0.002104 | lr 1.4067e-04 | norm: 0.0669 | dt: 5198.14ms | tok/sec: 3151.89\n",
      "step   929 | loss: 0.001396 | lr 1.3856e-04 | norm: 0.0468 | dt: 5193.89ms | tok/sec: 3154.47\n",
      "step   930 | loss: 0.001700 | lr 1.3647e-04 | norm: 0.0177 | dt: 5199.68ms | tok/sec: 3150.96\n",
      "step   931 | loss: 0.001166 | lr 1.3441e-04 | norm: 0.0346 | dt: 5202.61ms | tok/sec: 3149.19\n",
      "step   932 | loss: 0.001506 | lr 1.3237e-04 | norm: 0.0191 | dt: 5195.48ms | tok/sec: 3153.51\n",
      "step   933 | loss: 0.001208 | lr 1.3035e-04 | norm: 0.0106 | dt: 5202.37ms | tok/sec: 3149.33\n",
      "step   934 | loss: 0.001607 | lr 1.2836e-04 | norm: 0.0099 | dt: 5195.37ms | tok/sec: 3153.58\n",
      "step   935 | loss: 0.000784 | lr 1.2639e-04 | norm: 0.0214 | dt: 5207.45ms | tok/sec: 3146.26\n",
      "step   936 | loss: 0.001399 | lr 1.2445e-04 | norm: 0.0112 | dt: 5196.45ms | tok/sec: 3152.92\n",
      "step   937 | loss: 0.001200 | lr 1.2253e-04 | norm: 0.0100 | dt: 5207.58ms | tok/sec: 3146.18\n",
      "step   938 | loss: 0.001758 | lr 1.2064e-04 | norm: 0.0519 | dt: 5193.88ms | tok/sec: 3154.48\n",
      "step   939 | loss: 0.000919 | lr 1.1877e-04 | norm: 0.0083 | dt: 5204.04ms | tok/sec: 3148.32\n",
      "step   940 | loss: 0.001290 | lr 1.1693e-04 | norm: 0.0083 | dt: 5204.03ms | tok/sec: 3148.33\n",
      "step   941 | loss: 0.001213 | lr 1.1512e-04 | norm: 0.0096 | dt: 5198.38ms | tok/sec: 3151.75\n",
      "step   942 | loss: 0.001483 | lr 1.1333e-04 | norm: 0.0087 | dt: 5198.67ms | tok/sec: 3151.58\n",
      "step   943 | loss: 0.000877 | lr 1.1157e-04 | norm: 0.0122 | dt: 5193.58ms | tok/sec: 3154.66\n",
      "step   944 | loss: 0.001129 | lr 1.0983e-04 | norm: 0.0082 | dt: 5205.46ms | tok/sec: 3147.46\n",
      "step   945 | loss: 0.001308 | lr 1.0812e-04 | norm: 0.0107 | dt: 5194.51ms | tok/sec: 3154.10\n",
      "step   946 | loss: 0.001258 | lr 1.0644e-04 | norm: 0.0234 | dt: 5199.40ms | tok/sec: 3151.13\n",
      "step   947 | loss: 0.001051 | lr 1.0478e-04 | norm: 0.0081 | dt: 5200.61ms | tok/sec: 3150.40\n",
      "step   948 | loss: 0.000916 | lr 1.0315e-04 | norm: 0.0076 | dt: 5200.78ms | tok/sec: 3150.29\n",
      "step   949 | loss: 0.001382 | lr 1.0155e-04 | norm: 0.0102 | dt: 5194.49ms | tok/sec: 3154.11\n",
      "step   950 | loss: 0.001072 | lr 9.9982e-05 | norm: 0.0085 | dt: 5189.08ms | tok/sec: 3157.40\n",
      "step   951 | loss: 0.001180 | lr 9.8437e-05 | norm: 0.0087 | dt: 5194.05ms | tok/sec: 3154.38\n",
      "step   952 | loss: 0.000876 | lr 9.6921e-05 | norm: 0.0076 | dt: 5185.50ms | tok/sec: 3159.58\n",
      "step   953 | loss: 0.001288 | lr 9.5433e-05 | norm: 0.0102 | dt: 5185.74ms | tok/sec: 3159.43\n",
      "step   954 | loss: 0.001132 | lr 9.3973e-05 | norm: 0.0099 | dt: 5185.29ms | tok/sec: 3159.71\n",
      "step   955 | loss: 0.001275 | lr 9.2542e-05 | norm: 0.0088 | dt: 5190.85ms | tok/sec: 3156.32\n",
      "step   956 | loss: 0.000868 | lr 9.1140e-05 | norm: 0.0076 | dt: 5200.41ms | tok/sec: 3150.52\n",
      "step   957 | loss: 0.001121 | lr 8.9767e-05 | norm: 0.0097 | dt: 5194.93ms | tok/sec: 3153.84\n",
      "step   958 | loss: 0.001193 | lr 8.8423e-05 | norm: 0.0097 | dt: 5206.99ms | tok/sec: 3146.54\n",
      "step   959 | loss: 0.001147 | lr 8.7109e-05 | norm: 0.0082 | dt: 5205.91ms | tok/sec: 3147.19\n",
      "step   960 | loss: 0.000624 | lr 8.5824e-05 | norm: 0.0069 | dt: 5200.56ms | tok/sec: 3150.43\n",
      "step   961 | loss: 0.001358 | lr 8.4568e-05 | norm: 0.0093 | dt: 5209.55ms | tok/sec: 3144.99\n",
      "step   962 | loss: 0.001003 | lr 8.3343e-05 | norm: 0.0085 | dt: 5209.72ms | tok/sec: 3144.89\n",
      "step   963 | loss: 0.001415 | lr 8.2147e-05 | norm: 0.0085 | dt: 5223.20ms | tok/sec: 3136.78\n",
      "step   964 | loss: 0.000625 | lr 8.0982e-05 | norm: 0.0070 | dt: 5217.51ms | tok/sec: 3140.19\n",
      "step   965 | loss: 0.001214 | lr 7.9847e-05 | norm: 0.0080 | dt: 5214.84ms | tok/sec: 3141.80\n",
      "step   966 | loss: 0.000960 | lr 7.8742e-05 | norm: 0.0090 | dt: 5209.88ms | tok/sec: 3144.79\n",
      "step   967 | loss: 0.001409 | lr 7.7668e-05 | norm: 0.0089 | dt: 5199.84ms | tok/sec: 3150.86\n",
      "step   968 | loss: 0.000542 | lr 7.6624e-05 | norm: 0.0064 | dt: 5212.57ms | tok/sec: 3143.17\n",
      "step   969 | loss: 0.001195 | lr 7.5611e-05 | norm: 0.0083 | dt: 5209.96ms | tok/sec: 3144.75\n",
      "step   970 | loss: 0.001006 | lr 7.4629e-05 | norm: 0.0086 | dt: 5192.75ms | tok/sec: 3155.17\n",
      "step   971 | loss: 0.001363 | lr 7.3678e-05 | norm: 0.0084 | dt: 5191.08ms | tok/sec: 3156.19\n",
      "step   972 | loss: 0.000751 | lr 7.2759e-05 | norm: 0.0077 | dt: 5190.27ms | tok/sec: 3156.68\n",
      "step   973 | loss: 0.001105 | lr 7.1870e-05 | norm: 0.0075 | dt: 5200.31ms | tok/sec: 3150.58\n",
      "step   974 | loss: 0.001091 | lr 7.1013e-05 | norm: 0.0092 | dt: 5191.35ms | tok/sec: 3156.02\n",
      "step   975 | loss: 0.001329 | lr 7.0188e-05 | norm: 0.0080 | dt: 5202.54ms | tok/sec: 3149.23\n",
      "step   976 | loss: 0.000726 | lr 6.9394e-05 | norm: 0.0074 | dt: 5192.93ms | tok/sec: 3155.06\n",
      "step   977 | loss: 0.000968 | lr 6.8631e-05 | norm: 0.0070 | dt: 5189.82ms | tok/sec: 3156.95\n",
      "step   978 | loss: 0.001196 | lr 6.7901e-05 | norm: 0.0088 | dt: 5194.25ms | tok/sec: 3154.26\n",
      "step   979 | loss: 0.001111 | lr 6.7202e-05 | norm: 0.0065 | dt: 5198.85ms | tok/sec: 3151.46\n",
      "step   980 | loss: 0.000929 | lr 6.6535e-05 | norm: 0.0077 | dt: 5196.73ms | tok/sec: 3152.75\n",
      "step   981 | loss: 0.000798 | lr 6.5900e-05 | norm: 0.0072 | dt: 5196.10ms | tok/sec: 3153.13\n",
      "step   982 | loss: 0.001293 | lr 6.5297e-05 | norm: 0.0098 | dt: 5191.13ms | tok/sec: 3156.15\n",
      "step   983 | loss: 0.000954 | lr 6.4727e-05 | norm: 0.0079 | dt: 5193.46ms | tok/sec: 3154.74\n",
      "step   984 | loss: 0.001095 | lr 6.4188e-05 | norm: 0.0084 | dt: 5189.01ms | tok/sec: 3157.44\n",
      "step   985 | loss: 0.000779 | lr 6.3682e-05 | norm: 0.0073 | dt: 5189.45ms | tok/sec: 3157.18\n",
      "step   986 | loss: 0.001204 | lr 6.3209e-05 | norm: 0.0098 | dt: 5190.12ms | tok/sec: 3156.77\n",
      "step   987 | loss: 0.001038 | lr 6.2767e-05 | norm: 0.0095 | dt: 5193.97ms | tok/sec: 3154.43\n",
      "step   988 | loss: 0.001208 | lr 6.2359e-05 | norm: 0.0086 | dt: 5194.59ms | tok/sec: 3154.05\n",
      "step   989 | loss: 0.000782 | lr 6.1982e-05 | norm: 0.0072 | dt: 5202.48ms | tok/sec: 3149.27\n",
      "step   990 | loss: 0.001021 | lr 6.1639e-05 | norm: 0.0090 | dt: 5203.66ms | tok/sec: 3148.55\n",
      "step   991 | loss: 0.001137 | lr 6.1328e-05 | norm: 0.0094 | dt: 5214.71ms | tok/sec: 3141.88\n",
      "step   992 | loss: 0.001087 | lr 6.1049e-05 | norm: 0.0079 | dt: 5212.84ms | tok/sec: 3143.01\n",
      "step   993 | loss: 0.000555 | lr 6.0803e-05 | norm: 0.0066 | dt: 5215.54ms | tok/sec: 3141.38\n",
      "step   994 | loss: 0.001289 | lr 6.0590e-05 | norm: 0.0089 | dt: 5217.02ms | tok/sec: 3140.49\n",
      "step   995 | loss: 0.000963 | lr 6.0410e-05 | norm: 0.0084 | dt: 5214.48ms | tok/sec: 3142.02\n",
      "step   996 | loss: 0.001352 | lr 6.0262e-05 | norm: 0.0082 | dt: 5207.37ms | tok/sec: 3146.31\n",
      "step   997 | loss: 0.000565 | lr 6.0148e-05 | norm: 0.0067 | dt: 5210.15ms | tok/sec: 3144.63\n",
      "step   998 | loss: 0.001167 | lr 6.0066e-05 | norm: 0.0078 | dt: 5205.50ms | tok/sec: 3147.44\n",
      "validation loss: 10.5252\n",
      "HellaSwag accuracy: 2492/10042=0.2482\n",
      "rank 0 sample 0: Hello, I'm a language model, truffiness, water dendritic B enhancementscian through se it violent- raw s1980’s early\n",
      "rank 0 sample 1: Hello, I'm a language model, N/ the CarT was images appear relatively highbre. For twoet Offensive’s neighborhoods.\n",
      "A four\n",
      "rank 0 sample 2: Hello, I'm a language model, established by men, for memory1 hasrejected is violence research second baby foodA better work. For example, and\n",
      "rank 0 sample 3: Hello, I'm a language model, still been of green look. green holding aigens health: building even Research now to remain certain and hiv Fre\n",
      "step   999 | loss: 0.000920 | lr 6.0016e-05 | norm: 0.0089 | dt: 461896.35ms | tok/sec: 35.47\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "#from hellaswag import render_example, iterate_examples\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024  # max sequence length\n",
    "    vocab_size: int = 50257  # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 token\n",
    "    n_layer: int = 12  # number of layers\n",
    "    n_head: int = 12  # number of heads\n",
    "    n_embd: int = 768  # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "        config_args = {\n",
    "            'gpt2': dict(n_layer=12, n_head=12, n_embd=768),\n",
    "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),\n",
    "            'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),\n",
    "            'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257\n",
    "        config_args['block_size'] = 1024\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process:\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "        data_root = \"edu_fineweb10B\"\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        if master_process:\n",
    "            print(f\"found {len(shards)} shards for split {split}\")\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
    "        if buf.size(0) < B * T + 1:\n",
    "            if self.current_shard == len(self.shards) - 1:\n",
    "                pad_size = (B * T + 1) - buf.size(0)\n",
    "                buf = torch.cat([buf, torch.zeros(pad_size, dtype=torch.long)], dim=0)\n",
    "            else:\n",
    "                self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "                self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "                self.current_position = B * T * self.process_rank\n",
    "                return self.next_batch()\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_most_likely_row(tokens, mask, logits):\n",
    "    shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "    shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    flat_shift_tokens = shift_tokens.view(-1)\n",
    "    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "    shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "    shift_mask = (mask[..., 1:]).contiguous()\n",
    "    masked_shift_losses = shift_losses * shift_mask\n",
    "    sum_loss = masked_shift_losses.sum(dim=1)\n",
    "    avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "    pred_norm = avg_loss.argmin().item()\n",
    "    return pred_norm\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Change these lines if needed\n",
    "total_batch_size = 16384  # total batch size\n",
    "B = 16  # batch size\n",
    "T = 128  # sequence length\n",
    "\n",
    "\n",
    "\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
    "val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "use_compile = False\n",
    "if use_compile:\n",
    "    model = torch.compile(model)\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "raw_model = model.module if ddp else model\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "max_steps = 1000  # the number of steps\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
    "\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "with open(log_file, \"w\") as f:\n",
    "    pass\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    if step % 250 == 0 or last_step:\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.SUM)\n",
    "        if master_process:\n",
    "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "            if step > 0 and (step % 5000 == 0 or last_step):\n",
    "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'config': raw_model.config,\n",
    "                    'step': step,\n",
    "                    'val_loss': val_loss_accum.item()\n",
    "                }\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    if (step % 250 == 0 or last_step) and (not use_compile):\n",
    "        num_correct_norm = 0\n",
    "        num_total = 0\n",
    "        for i, example in enumerate(iterate_examples(\"val\")):\n",
    "            if i % ddp_world_size != ddp_rank:\n",
    "                continue\n",
    "            _, tokens, mask, label = render_example(example)\n",
    "            tokens = tokens.to(device)\n",
    "            mask = mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(tokens)\n",
    "                pred_norm = get_most_likely_row(tokens, mask, logits)\n",
    "            num_total += 1\n",
    "            num_correct_norm += int(pred_norm == label)\n",
    "        if ddp:\n",
    "            num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n",
    "            num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n",
    "            dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n",
    "            dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n",
    "            num_total = num_total.item()\n",
    "            num_correct_norm = num_correct_norm.item()\n",
    "        acc_norm = num_correct_norm / num_total\n",
    "        if master_process:\n",
    "            print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n",
    "\n",
    "    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n",
    "        model.eval()\n",
    "        num_return_sequences = 4\n",
    "        max_length = 32\n",
    "        tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "        xgen = tokens.to(device)\n",
    "        sample_rng = torch.Generator(device=device)\n",
    "        sample_rng.manual_seed(42 + ddp_rank)\n",
    "        while xgen.size(1) < max_length:\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(xgen)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng)\n",
    "                xcol = torch.gather(topk_indices, -1, ix)\n",
    "                xgen = torch.cat((xgen, xcol), dim=1)\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = xgen[i, :max_length].tolist()\n",
    "            decoded = enc.decode(tokens)\n",
    "            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.SUM)\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    if master_process:\n",
    "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
